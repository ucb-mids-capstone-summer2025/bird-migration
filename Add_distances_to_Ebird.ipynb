{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H89_dLkT7r7e"
   },
   "source": [
    "# Add 'i95_distance', 'station_distance' and 'station_id\" to Ebird datasets\n",
    "- To used with datasets pre-cleaned using AWS SageMaker Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:16.026101Z",
     "iopub.status.busy": "2025-06-20T20:59:16.025784Z",
     "iopub.status.idle": "2025-06-20T20:59:16.030778Z",
     "shell.execute_reply": "2025-06-20T20:59:16.030020Z",
     "shell.execute_reply.started": "2025-06-20T20:59:16.026075Z"
    },
    "id": "DfzBXsw08fR5",
    "outputId": "2ab254e0-aa3b-40f9-8181-329c715bdb91"
   },
   "outputs": [],
   "source": [
    "# # mount google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 915
    },
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:16.035903Z",
     "iopub.status.busy": "2025-06-20T20:59:16.035534Z",
     "iopub.status.idle": "2025-06-20T20:59:20.515619Z",
     "shell.execute_reply": "2025-06-20T20:59:20.514795Z",
     "shell.execute_reply.started": "2025-06-20T20:59:16.035878Z"
    },
    "id": "4iCuMSW38zpo",
    "outputId": "aeb98458-0fd9-4449-a660-7427f8562756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /opt/conda/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: shapely in /opt/conda/lib/python3.12/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /opt/conda/lib/python3.12/site-packages (from geopandas) (2.3.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /opt/conda/lib/python3.12/site-packages (from geopandas) (0.11.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from geopandas) (24.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from geopandas) (2.3.0)\n",
      "Requirement already satisfied: pyproj>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from geopandas) (3.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas>=2.0.0->geopandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=2.0.0->geopandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=2.0.0->geopandas) (2025.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from pyogrio>=0.7.2->geopandas) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->geopandas) (1.17.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# # required installs\n",
    "!pip install geopandas shapely\n",
    "!pip install --upgrade pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:20.517938Z",
     "iopub.status.busy": "2025-06-20T20:59:20.517603Z",
     "iopub.status.idle": "2025-06-20T20:59:22.183865Z",
     "shell.execute_reply": "2025-06-20T20:59:22.182898Z",
     "shell.execute_reply.started": "2025-06-20T20:59:20.517909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in /opt/conda/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /opt/conda/lib/python3.12/site-packages (from geopy) (2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:22.185202Z",
     "iopub.status.busy": "2025-06-20T20:59:22.184904Z",
     "iopub.status.idle": "2025-06-20T20:59:24.221376Z",
     "shell.execute_reply": "2025-06-20T20:59:24.220623Z",
     "shell.execute_reply.started": "2025-06-20T20:59:22.185167Z"
    },
    "id": "QpRlqTOZ8uqR"
   },
   "outputs": [],
   "source": [
    "# required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import zipfile\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import gc\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely.ops import nearest_points\n",
    "from typing import Optional\n",
    "from geopy.distance import geodesic\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zulQPkeK9TsU"
   },
   "source": [
    "scipy library requires an older numpy version. Unfortunately, using the old numpy inhibits the use of pandas.  Run the following only if calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:24.222929Z",
     "iopub.status.busy": "2025-06-20T20:59:24.222508Z",
     "iopub.status.idle": "2025-06-20T20:59:24.225956Z",
     "shell.execute_reply": "2025-06-20T20:59:24.225239Z",
     "shell.execute_reply.started": "2025-06-20T20:59:24.222888Z"
    },
    "id": "o7-aC51496IW"
   },
   "outputs": [],
   "source": [
    "# ## Run this first and restart the notebook - resolves scipy-numpy discrepency ###\n",
    "# !pip uninstall -y numpy scipy\n",
    "# !pip install numpy==1.24.4 scipy==1.10.1\n",
    "\n",
    "# # Running this cell will require you to restart the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:24.228248Z",
     "iopub.status.busy": "2025-06-20T20:59:24.227650Z",
     "iopub.status.idle": "2025-06-20T20:59:24.232004Z",
     "shell.execute_reply": "2025-06-20T20:59:24.231340Z",
     "shell.execute_reply.started": "2025-06-20T20:59:24.228224Z"
    },
    "id": "RQFgE-K49LoV"
   },
   "outputs": [],
   "source": [
    "# # imports for\n",
    "# from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wCseRnL8Zfk"
   },
   "source": [
    "### Dataset check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:24.234502Z",
     "iopub.status.busy": "2025-06-20T20:59:24.232876Z",
     "iopub.status.idle": "2025-06-20T20:59:24.849277Z",
     "shell.execute_reply": "2025-06-20T20:59:24.848432Z",
     "shell.execute_reply.started": "2025-06-20T20:59:24.234477Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "from botocore.exceptions import ClientError\n",
    "from shapely.geometry import Point, Polygon\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Union, Tuple\n",
    "import json\n",
    "from ipywidgets import interact, FloatSlider, Layout\n",
    "import ipywidgets as widgets\n",
    "from shapely.vectorized import contains\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:24.850637Z",
     "iopub.status.busy": "2025-06-20T20:59:24.850174Z",
     "iopub.status.idle": "2025-06-20T20:59:25.449840Z",
     "shell.execute_reply": "2025-06-20T20:59:25.449014Z",
     "shell.execute_reply.started": "2025-06-20T20:59:24.850614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>station_id</th>\n",
       "      <th>year_record</th>\n",
       "      <th>month_record</th>\n",
       "      <th>day_record</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>latitude_0</th>\n",
       "      <th>longitude_0</th>\n",
       "      <th>station_location</th>\n",
       "      <th>state</th>\n",
       "      <th>...</th>\n",
       "      <th>OBSERVATION DATE_month</th>\n",
       "      <th>OBSERVATION DATE_day</th>\n",
       "      <th>TimeObservationStarted_hour</th>\n",
       "      <th>TimeObservationStarted_minute</th>\n",
       "      <th>distance_to_station_160005</th>\n",
       "      <th>distance_to_station_140004</th>\n",
       "      <th>distance_to_station_160308</th>\n",
       "      <th>distance_to_station_060170</th>\n",
       "      <th>distance_to_station_792625</th>\n",
       "      <th>assigned_station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>160005</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>38.13873</td>\n",
       "      <td>-77.50837</td>\n",
       "      <td>5.85 S RAMP FR RT 1                           ...</td>\n",
       "      <td>VA</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5.323592</td>\n",
       "      <td>40.842887</td>\n",
       "      <td>33.085488</td>\n",
       "      <td>7.701909</td>\n",
       "      <td>22.943019</td>\n",
       "      <td>160005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>160005</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>38.13873</td>\n",
       "      <td>-77.50837</td>\n",
       "      <td>5.85 S RAMP FR RT 1                           ...</td>\n",
       "      <td>VA</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5.323592</td>\n",
       "      <td>40.842887</td>\n",
       "      <td>33.085488</td>\n",
       "      <td>7.701909</td>\n",
       "      <td>22.943019</td>\n",
       "      <td>160005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>160005</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>38.13873</td>\n",
       "      <td>-77.50837</td>\n",
       "      <td>5.85 S RAMP FR RT 1                           ...</td>\n",
       "      <td>VA</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5.323592</td>\n",
       "      <td>40.842887</td>\n",
       "      <td>33.085488</td>\n",
       "      <td>7.701909</td>\n",
       "      <td>22.943019</td>\n",
       "      <td>160005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>160005</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>38.13873</td>\n",
       "      <td>-77.50837</td>\n",
       "      <td>5.85 S RAMP FR RT 1                           ...</td>\n",
       "      <td>VA</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5.323592</td>\n",
       "      <td>40.842887</td>\n",
       "      <td>33.085488</td>\n",
       "      <td>7.701909</td>\n",
       "      <td>22.943019</td>\n",
       "      <td>160005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>160005</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>38.13873</td>\n",
       "      <td>-77.50837</td>\n",
       "      <td>5.85 S RAMP FR RT 1                           ...</td>\n",
       "      <td>VA</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5.323592</td>\n",
       "      <td>40.842887</td>\n",
       "      <td>33.085488</td>\n",
       "      <td>7.701909</td>\n",
       "      <td>22.943019</td>\n",
       "      <td>160005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   state_code  station_id  year_record  month_record  day_record  day_of_week  \\\n",
       "0          51      160005         2023             6          30            6   \n",
       "1          51      160005         2023             6          29            5   \n",
       "2          51      160005         2023             6          28            4   \n",
       "3          51      160005         2023             6          27            3   \n",
       "4          51      160005         2023             6          26            2   \n",
       "\n",
       "   latitude_0  longitude_0                                   station_location  \\\n",
       "0    38.13873    -77.50837  5.85 S RAMP FR RT 1                           ...   \n",
       "1    38.13873    -77.50837  5.85 S RAMP FR RT 1                           ...   \n",
       "2    38.13873    -77.50837  5.85 S RAMP FR RT 1                           ...   \n",
       "3    38.13873    -77.50837  5.85 S RAMP FR RT 1                           ...   \n",
       "4    38.13873    -77.50837  5.85 S RAMP FR RT 1                           ...   \n",
       "\n",
       "  state  ...  OBSERVATION DATE_month  OBSERVATION DATE_day  \\\n",
       "0    VA  ...                       0                    19   \n",
       "1    VA  ...                       0                    19   \n",
       "2    VA  ...                       0                    19   \n",
       "3    VA  ...                       0                    19   \n",
       "4    VA  ...                       0                    19   \n",
       "\n",
       "   TimeObservationStarted_hour  TimeObservationStarted_minute  \\\n",
       "0                           14                              0   \n",
       "1                           14                              0   \n",
       "2                           14                              0   \n",
       "3                           14                              0   \n",
       "4                           14                              0   \n",
       "\n",
       "   distance_to_station_160005  distance_to_station_140004  \\\n",
       "0                    5.323592                   40.842887   \n",
       "1                    5.323592                   40.842887   \n",
       "2                    5.323592                   40.842887   \n",
       "3                    5.323592                   40.842887   \n",
       "4                    5.323592                   40.842887   \n",
       "\n",
       "  distance_to_station_160308 distance_to_station_060170  \\\n",
       "0                  33.085488                   7.701909   \n",
       "1                  33.085488                   7.701909   \n",
       "2                  33.085488                   7.701909   \n",
       "3                  33.085488                   7.701909   \n",
       "4                  33.085488                   7.701909   \n",
       "\n",
       "   distance_to_station_792625  assigned_station  \n",
       "0                   22.943019            160005  \n",
       "1                   22.943019            160005  \n",
       "2                   22.943019            160005  \n",
       "3                   22.943019            160005  \n",
       "4                   22.943019            160005  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize boto3 session and get credentials\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "# Initialize S3 filesystem with boto3 credentials\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    key=credentials.access_key,\n",
    "    secret=credentials.secret_key,\n",
    "    token=credentials.token,  # This will be included if using temporary credentials\n",
    "    anon=False\n",
    ")\n",
    "# Read the parquet file\n",
    "s3_path = 's3://birdsbucker/Pipeline_Outputs/output_6ff24b78-4d86-4453-9a84-032d30a10854/part-00000-a03252eb-99f3-41d5-b2a1-4f0abd303539-c000.parquet'\n",
    "birds = pd.read_parquet(s3_path, filesystem=s3)\n",
    "\n",
    "birds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:25.451229Z",
     "iopub.status.busy": "2025-06-20T20:59:25.450810Z",
     "iopub.status.idle": "2025-06-20T20:59:25.455078Z",
     "shell.execute_reply": "2025-06-20T20:59:25.454445Z",
     "shell.execute_reply.started": "2025-06-20T20:59:25.451198Z"
    },
    "id": "SZfoRji48pc6"
   },
   "outputs": [],
   "source": [
    "# data read\n",
    "# vs = pd.read_parquet(\"/content/VA_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:25.456402Z",
     "iopub.status.busy": "2025-06-20T20:59:25.455997Z",
     "iopub.status.idle": "2025-06-20T20:59:25.460558Z",
     "shell.execute_reply": "2025-06-20T20:59:25.459806Z",
     "shell.execute_reply.started": "2025-06-20T20:59:25.456264Z"
    },
    "id": "WUqu30aX7vTf",
    "outputId": "f89fdc97-8efa-455c-8008-022998b13ee5"
   },
   "outputs": [],
   "source": [
    "# vs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:25.462074Z",
     "iopub.status.busy": "2025-06-20T20:59:25.461728Z",
     "iopub.status.idle": "2025-06-20T20:59:25.465335Z",
     "shell.execute_reply": "2025-06-20T20:59:25.464541Z",
     "shell.execute_reply.started": "2025-06-20T20:59:25.462047Z"
    },
    "id": "7rNZMavo7rXW",
    "outputId": "4f5c3ff2-36b9-4112-c4b0-4c98f3b805f7"
   },
   "outputs": [],
   "source": [
    "# vs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:25.466694Z",
     "iopub.status.busy": "2025-06-20T20:59:25.466400Z",
     "iopub.status.idle": "2025-06-20T20:59:25.470082Z",
     "shell.execute_reply": "2025-06-20T20:59:25.469348Z",
     "shell.execute_reply.started": "2025-06-20T20:59:25.466665Z"
    },
    "id": "SEno9w_VXWDN",
    "outputId": "021c9ba4-5453-4205-83b0-4a1e3a89cdeb"
   },
   "outputs": [],
   "source": [
    "# print(vs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:25.471826Z",
     "iopub.status.busy": "2025-06-20T20:59:25.471547Z",
     "iopub.status.idle": "2025-06-20T20:59:25.475688Z",
     "shell.execute_reply": "2025-06-20T20:59:25.475079Z",
     "shell.execute_reply.started": "2025-06-20T20:59:25.471806Z"
    },
    "id": "yxN96SvFaOMw",
    "outputId": "08fac314-aabf-4bf8-f509-ccba8fee8f7c"
   },
   "outputs": [],
   "source": [
    "# vs.LATITUDE_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyYUyv1RK_Yg"
   },
   "source": [
    "### Calculates distance between bird observation point to I95 - nearest point on a line between the two nearest I95 geo coordinates.\n",
    "Adds 'i95_distance' to Ebirds dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:25.477116Z",
     "iopub.status.busy": "2025-06-20T20:59:25.476685Z",
     "iopub.status.idle": "2025-06-20T20:59:25.480863Z",
     "shell.execute_reply": "2025-06-20T20:59:25.480129Z",
     "shell.execute_reply.started": "2025-06-20T20:59:25.477087Z"
    },
    "id": "DqpRWrJO_g6e"
   },
   "outputs": [],
   "source": [
    "# # Read in Ebird data if not too large or loa\n",
    "# birds = pd.read_csv('DATASET', nrows=1000)\n",
    "# print(birds.shape)\n",
    "# print(birds.columns)\n",
    "# print(birds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:25.483647Z",
     "iopub.status.busy": "2025-06-20T20:59:25.483225Z",
     "iopub.status.idle": "2025-06-20T20:59:25.930108Z",
     "shell.execute_reply": "2025-06-20T20:59:25.929401Z",
     "shell.execute_reply.started": "2025-06-20T20:59:25.483622Z"
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "        \n",
    "# Get the object from S3\n",
    "response = s3_client.get_object(Bucket='birdsbucker', Key='i95_coordinates.csv')\n",
    "\n",
    "# Read the CSV content with additional options\n",
    "i95_coordinates = pd.read_csv(io.BytesIO(response['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:25.931536Z",
     "iopub.status.busy": "2025-06-20T20:59:25.930913Z",
     "iopub.status.idle": "2025-06-20T20:59:25.938436Z",
     "shell.execute_reply": "2025-06-20T20:59:25.937844Z",
     "shell.execute_reply.started": "2025-06-20T20:59:25.931508Z"
    },
    "id": "Elwm3ZhyMYe1",
    "outputId": "88728cf8-bdc0-4db3-db9e-7bf2485de6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55654, 10)\n",
      "Index(['Way_ID', 'Segment_Number', 'Point_Order', 'Longitude', 'Latitude',\n",
      "       'Highway_Type', 'Route_Ref', 'Max_Speed', 'Combined_Path',\n",
      "       'Overall_Sequence'],\n",
      "      dtype='object')\n",
      "     Way_ID  Segment_Number  Point_Order  Longitude   Latitude Highway_Type  \\\n",
      "0  way/2059               0            0 -77.451046  37.680565     motorway   \n",
      "1  way/2059               0            1 -77.451240  37.681075     motorway   \n",
      "2  way/2059               0            2 -77.451470  37.681770     motorway   \n",
      "3  way/2059               0            3 -77.451679  37.682531     motorway   \n",
      "4  way/2059               0            4 -77.451867  37.683562     motorway   \n",
      "\n",
      "  Route_Ref Max_Speed Combined_Path  Overall_Sequence  \n",
      "0      I 95    65 mph           0_0                 0  \n",
      "1      I 95    65 mph           0_1                 1  \n",
      "2      I 95    65 mph           0_2                 2  \n",
      "3      I 95    65 mph           0_3                 3  \n",
      "4      I 95    65 mph           0_4                 4  \n"
     ]
    }
   ],
   "source": [
    "# i95_coordinates = pd.read_csv('i95_coordinates.csv')\n",
    "print(i95_coordinates.shape)\n",
    "print(i95_coordinates.columns)\n",
    "print(i95_coordinates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:25.939622Z",
     "iopub.status.busy": "2025-06-20T20:59:25.939393Z",
     "iopub.status.idle": "2025-06-20T20:59:25.965379Z",
     "shell.execute_reply": "2025-06-20T20:59:25.964682Z",
     "shell.execute_reply.started": "2025-06-20T20:59:25.939601Z"
    },
    "id": "Pk81Vwh6OddC"
   },
   "outputs": [],
   "source": [
    "i95_sorted = i95_coordinates.sort_values(['Overall_Sequence'])\n",
    "i95_coords = list(zip(i95_sorted['Latitude'], i95_sorted['Longitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:25.966583Z",
     "iopub.status.busy": "2025-06-20T20:59:25.966136Z",
     "iopub.status.idle": "2025-06-20T20:59:27.673121Z",
     "shell.execute_reply": "2025-06-20T20:59:27.672095Z",
     "shell.execute_reply.started": "2025-06-20T20:59:25.966558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in /opt/conda/lib/python3.12/site-packages (0.20.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from folium) (0.8.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in /opt/conda/lib/python3.12/site-packages (from folium) (3.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from folium) (2.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from folium) (2.32.3)\n",
      "Requirement already satisfied: xyzservices in /opt/conda/lib/python3.12/site-packages (from folium) (2025.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2>=2.9->folium) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->folium) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->folium) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->folium) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:59:27.675085Z",
     "iopub.status.busy": "2025-06-20T20:59:27.674255Z",
     "iopub.status.idle": "2025-06-20T21:00:55.408859Z",
     "shell.execute_reply": "2025-06-20T21:00:55.408029Z",
     "shell.execute_reply.started": "2025-06-20T20:59:27.675054Z"
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Create a base map centered on the mean coordinates\n",
    "center_lat = i95_coordinates['Latitude'].mean()\n",
    "center_lon = i95_coordinates['Longitude'].mean()\n",
    "m = folium.Map(location=[center_lat, center_lon], \n",
    "               zoom_start=8,\n",
    "               tiles='cartodbpositron')  # Different map style\n",
    "\n",
    "# Add markers for each point\n",
    "for idx, row in i95_coordinates.iterrows():\n",
    "    # You can customize the color based on some condition\n",
    "    folium.CircleMarker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        radius=6,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.7,\n",
    "        popup=folium.Popup(f\"Point {idx}\", parse_html=True),\n",
    "        tooltip=f\"Click for more info\"\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add a title\n",
    "title_html = '''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\">\n",
    "             <b>I-95 Coordinates</b>\n",
    "             </h3>\n",
    "             '''\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "m.save('i95_route points.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T21:00:55.410012Z",
     "iopub.status.busy": "2025-06-20T21:00:55.409670Z",
     "iopub.status.idle": "2025-06-20T21:00:55.414728Z",
     "shell.execute_reply": "2025-06-20T21:00:55.414092Z",
     "shell.execute_reply.started": "2025-06-20T21:00:55.409991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['state_code', 'station_id', 'year_record', 'month_record', 'day_record',\n",
       "       'day_of_week', 'latitude_0', 'longitude_0', 'station_location', 'state',\n",
       "       'daily_avg_noise', 'peak_hour_noise', 'overnight_noise',\n",
       "       'rush_hour_noise', 'total_daily_volume', 'OBSERVATION COUNT',\n",
       "       'STATE CODE', 'LOCALITY TYPE', 'LATITUDE_1', 'LONGITUDE_1',\n",
       "       'OBSERVATION DATE', 'OBSERVATION DATE_year', 'OBSERVATION DATE_month',\n",
       "       'OBSERVATION DATE_day', 'TimeObservationStarted_hour',\n",
       "       'TimeObservationStarted_minute', 'distance_to_station_160005',\n",
       "       'distance_to_station_140004', 'distance_to_station_160308',\n",
       "       'distance_to_station_060170', 'distance_to_station_792625',\n",
       "       'assigned_station'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T21:00:55.416011Z",
     "iopub.status.busy": "2025-06-20T21:00:55.415801Z",
     "iopub.status.idle": "2025-06-20T21:00:55.440196Z",
     "shell.execute_reply": "2025-06-20T21:00:55.439377Z",
     "shell.execute_reply.started": "2025-06-20T21:00:55.415990Z"
    },
    "id": "0UlyE5h4c5kL"
   },
   "outputs": [],
   "source": [
    "\n",
    "# import logging\n",
    "# import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# import numpy as np\n",
    "# from shapely.geometry import Point, LineString\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "# from pathlib import Path\n",
    "# import gc\n",
    "# from typing import Optional, List, Dict, Any\n",
    "\n",
    "### Modify file names before running ###\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class OptimizedParquetBatchProcessor:\n",
    "    \"\"\"\n",
    "    Heavily optimized processor for large parquet files using GeoPandas, spatial indexing,\n",
    "    and vectorized operations with memory-efficient parquet streaming.\n",
    "    Expected 10-100x performance improvement with better memory management.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_file: str = \"cleaned_ebird_file.pq\",\n",
    "                 output_file: str = \"new_file.pq\",\n",
    "                 batch_size: int = 50000,\n",
    "                 distance_threshold: Optional[float] = None,\n",
    "                 i95_coords: Optional[List] = None,\n",
    "                #  columns_to_keep: Optional[List[str]] = None,\n",
    "                 use_compression: str = 'snappy'):\n",
    "\n",
    "        self.input_file = Path(input_file)\n",
    "        self.output_file = Path(output_file)\n",
    "        self.batch_size = batch_size\n",
    "        self.distance_threshold = distance_threshold\n",
    "        # self.columns_to_keep = columns_to_keep\n",
    "        self.use_compression = use_compression\n",
    "\n",
    "        # Validate input file\n",
    "        if not self.input_file.exists():\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "        # Setup parquet file metadata\n",
    "        self._setup_parquet_metadata()\n",
    "\n",
    "        # Pre-process I-95 coordinates into optimized spatial structures\n",
    "        self._setup_highway_geometry(i95_coords)\n",
    "\n",
    "        # Statistics tracking\n",
    "        self.total_rows_processed = 0\n",
    "        self.total_rows_saved = 0\n",
    "        self.batch_count = 0\n",
    "        self.total_file_size = 0\n",
    "\n",
    "    def _setup_parquet_metadata(self):\n",
    "        \"\"\"Get parquet file metadata for optimization\"\"\"\n",
    "        try:\n",
    "            # Read parquet metadata\n",
    "            parquet_file = pq.ParquetFile(self.input_file)\n",
    "            self.parquet_metadata = parquet_file.metadata\n",
    "            self.parquet_schema = parquet_file.schema\n",
    "            self.total_rows = self.parquet_metadata.num_rows\n",
    "\n",
    "            logger.info(f\"Parquet file info:\")\n",
    "            logger.info(f\"  Total rows: {self.total_rows:,}\")\n",
    "            logger.info(f\"  Number of row groups: {self.parquet_metadata.num_row_groups}\")\n",
    "            logger.info(f\"  Columns: {len(self.parquet_schema)}\")\n",
    "\n",
    "            # Get file size\n",
    "            self.total_file_size = self.input_file.stat().st_size / (1024**3)  # GB\n",
    "            logger.info(f\"  File size: {self.total_file_size:.2f} GB\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading parquet metadata: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_highway_geometry(self, i95_coords):\n",
    "        \"\"\"Convert I-95 coordinates to optimized spatial structures\"\"\"\n",
    "        if not i95_coords:\n",
    "            raise ValueError(\"I-95 coordinates must be provided\")\n",
    "\n",
    "        logger.info(\"Setting up highway geometry with spatial indexing...\")\n",
    "\n",
    "        # Create LineString geometry from coordinates\n",
    "        # i95_coords are (lat,lon) but LineString expects (lon,lat), so swap them\n",
    "        self.highway_line = LineString([(lon, lat) for lat, lon in i95_coords])\n",
    "\n",
    "        # Create GeoDataFrame for the highway with spatial index\n",
    "        highway_gdf = gpd.GeoDataFrame([1], geometry=[self.highway_line], crs='EPSG:4326')\n",
    "\n",
    "        # Convert to projected CRS for accurate distance calculations (UTM Zone 18N)\n",
    "        self.highway_gdf_projected = highway_gdf.to_crs('EPSG:32618')\n",
    "        self.highway_line_projected = self.highway_gdf_projected.geometry.iloc[0]\n",
    "\n",
    "        logger.info(\"Highway geometry setup complete\")\n",
    "\n",
    "    def calculate_distances_vectorized(self, obs_gdf: gpd.GeoDataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Vectorized distance calculation using GeoPandas\n",
    "        This is the key optimization - processes all points at once\n",
    "        \"\"\"\n",
    "        # Project observations to same CRS as highway (UTM Zone 18N)\n",
    "        obs_projected = obs_gdf.to_crs('EPSG:32618')\n",
    "\n",
    "        # Vectorized distance calculation to highway line\n",
    "        distances_meters = obs_projected.geometry.distance(self.highway_line_projected)\n",
    "\n",
    "        # Convert meters to miles\n",
    "        distances_miles = distances_meters * 0.000621371\n",
    "\n",
    "        return distances_miles.values\n",
    "\n",
    "    def process_batch_optimized(self, batch_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Optimized batch processing using vectorized operations\"\"\"\n",
    "\n",
    "        # Early return if empty batch\n",
    "        if batch_df.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Filter out rows with invalid coordinates early\n",
    "        valid_coords = batch_df.dropna(subset=['LATITUDE_1', 'LONGITUDE_1'])\n",
    "\n",
    "        if len(valid_coords) == 0:\n",
    "            logger.warning(\"No valid coordinates in batch\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Convert to numeric and filter realistic coordinate ranges\n",
    "        valid_coords = valid_coords.copy()\n",
    "\n",
    "        # Use more efficient numeric conversion\n",
    "        coord_cols = ['LATITUDE_1', 'LONGITUDE_1']\n",
    "        for col in coord_cols:\n",
    "            if valid_coords[col].dtype == 'object':\n",
    "                valid_coords[col] = pd.to_numeric(valid_coords[col], errors='coerce')\n",
    "\n",
    "        # Filter to reasonable coordinate bounds (roughly continental US)\n",
    "        coord_filter = (\n",
    "            (valid_coords['LATITUDE_1'].between(24, 50)) &\n",
    "            (valid_coords['LONGITUDE_1'].between(-130, -65))\n",
    "        )\n",
    "        valid_coords = valid_coords[coord_filter]\n",
    "\n",
    "        if len(valid_coords) == 0:\n",
    "            logger.warning(\"No valid coordinates after filtering\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Create GeoDataFrame from observations\n",
    "        geometry = gpd.points_from_xy(valid_coords['LONGITUDE_1'], valid_coords['LATITUDE_1'])\n",
    "        obs_gdf = gpd.GeoDataFrame(valid_coords, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "        # Calculate distances using vectorized operation\n",
    "        distances = self.calculate_distances_vectorized(obs_gdf)\n",
    "\n",
    "        # Add distances to dataframe\n",
    "        result_df = valid_coords.copy()\n",
    "        result_df['i95_distance'] = distances\n",
    "\n",
    "        # Apply distance filter if specified\n",
    "        if self.distance_threshold is not None:\n",
    "            result_df = result_df[result_df['i95_distance'] <= self.distance_threshold]\n",
    "\n",
    "        # # Select only specified columns if provided\n",
    "        # if self.columns_to_keep:\n",
    "        #     available_cols = [col for col in self.columns_to_keep + ['i95_distance']\n",
    "        #                     if col in result_df.columns]\n",
    "        #     result_df = result_df[available_cols]\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def _process_one_batch(self, batch_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process a single batch with logging and memory management\"\"\"\n",
    "        self.batch_count += 1\n",
    "        self.total_rows_processed += len(batch_df)\n",
    "\n",
    "        if self.batch_count % 10 == 0:  # Log every 10th batch to reduce noise\n",
    "            progress = (self.total_rows_processed / self.total_rows) * 100\n",
    "            logger.info(f\"Processing batch {self.batch_count} ({progress:.1f}% complete) \"\n",
    "                       f\"with {len(batch_df)} rows\")\n",
    "\n",
    "        processed_batch = self.process_batch_optimized(batch_df)\n",
    "        self.total_rows_saved += len(processed_batch)\n",
    "\n",
    "        # Force garbage collection periodically\n",
    "        if self.batch_count % 50 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "        return processed_batch\n",
    "\n",
    "    def _read_parquet_in_batches(self):\n",
    "        \"\"\"Generator to read parquet file in batches efficiently\"\"\"\n",
    "        try:\n",
    "            # Use pyarrow for more efficient reading\n",
    "            parquet_file = pq.ParquetFile(self.input_file)\n",
    "\n",
    "            # Read in batches using row groups when possible\n",
    "            for batch in parquet_file.iter_batches(batch_size=self.batch_size):\n",
    "                # Convert to pandas DataFrame\n",
    "                df = batch.to_pandas()\n",
    "                yield df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading parquet in batches: {e}\")\n",
    "            # Fallback to pandas chunking\n",
    "            logger.info(\"Falling back to pandas chunking...\")\n",
    "            try:\n",
    "                # Read the entire file and chunk it manually since pandas doesn't support chunksize for parquet\n",
    "                df = pd.read_parquet(self.input_file)\n",
    "                for i in range(0, len(df), self.batch_size):\n",
    "                    yield df.iloc[i:i+self.batch_size]\n",
    "                del df  # Free memory\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Fallback also failed: {e2}\")\n",
    "                raise\n",
    "\n",
    "    def _write_parquet_batch(self, df: pd.DataFrame, is_first_batch: bool = False):\n",
    "        \"\"\"Write batch to parquet file efficiently\"\"\"\n",
    "        if df.empty:\n",
    "            return\n",
    "\n",
    "        # Convert to Arrow Table for efficient writing\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        if is_first_batch:\n",
    "            # Create new file with compatible parameters\n",
    "            writer = pq.ParquetWriter(\n",
    "                self.output_file,\n",
    "                table.schema,\n",
    "                compression=self.use_compression,\n",
    "                use_dictionary=True  # Enable dictionary encoding\n",
    "            )\n",
    "            self._parquet_writer = writer\n",
    "\n",
    "        # Write the batch\n",
    "        self._parquet_writer.write_table(table)\n",
    "\n",
    "    def run_pipeline(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run the complete optimized pipeline for parquet files\"\"\"\n",
    "        logger.info(f\"Starting optimized parquet pipeline: {self.input_file}\")\n",
    "        logger.info(f\"Batch size: {self.batch_size:,}\")\n",
    "        logger.info(f\"Distance threshold: {self.distance_threshold}\")\n",
    "        logger.info(f\"Expected batches: {(self.total_rows // self.batch_size) + 1}\")\n",
    "\n",
    "        start_time = pd.Timestamp.now()\n",
    "        first_batch = True\n",
    "\n",
    "        try:\n",
    "            # Process file in batches\n",
    "            for batch_df in self._read_parquet_in_batches():\n",
    "                processed_batch = self._process_one_batch(batch_df)\n",
    "\n",
    "                if not processed_batch.empty:\n",
    "                    self._write_parquet_batch(processed_batch, is_first_batch=first_batch)\n",
    "                    first_batch = False\n",
    "\n",
    "            # Close the parquet writer\n",
    "            if hasattr(self, '_parquet_writer'):\n",
    "                self._parquet_writer.close()\n",
    "                logger.info(f\"Saved {self.total_rows_saved:,} rows to {self.output_file}\")\n",
    "            else:\n",
    "                logger.warning(\"No data to save\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline error: {str(e)}\")\n",
    "            # Clean up partial file\n",
    "            if self.output_file.exists():\n",
    "                self.output_file.unlink()\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if hasattr(self, '_parquet_writer'):\n",
    "                try:\n",
    "                    self._parquet_writer.close()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        end_time = pd.Timestamp.now()\n",
    "        processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'total_rows_processed': self.total_rows_processed,\n",
    "            'total_rows_saved': self.total_rows_saved,\n",
    "            'total_batches': self.batch_count,\n",
    "            'input_file': str(self.input_file),\n",
    "            'output_file': str(self.output_file),\n",
    "            'input_file_size_gb': self.total_file_size,\n",
    "            'output_file_size_gb': self.output_file.stat().st_size / (1024**3) if self.output_file.exists() else 0,\n",
    "            'processing_time_seconds': processing_time,\n",
    "            'processing_time_formatted': str(pd.Timedelta(seconds=processing_time)),\n",
    "            'rows_per_second': self.total_rows_processed / processing_time if processing_time > 0 else 0,\n",
    "            'filter_efficiency_percent': (self.total_rows_saved / self.total_rows_processed * 100)\n",
    "                                        if self.total_rows_processed > 0 else 0,\n",
    "            'compression_ratio': (self.total_file_size / (self.output_file.stat().st_size / (1024**3)))\n",
    "                               if self.output_file.exists() and self.output_file.stat().st_size > 0 else 0\n",
    "        }\n",
    "\n",
    "        logger.info(\"Optimized parquet pipeline completed!\")\n",
    "        logger.info(f\"Processing time: {stats['processing_time_formatted']}\")\n",
    "        logger.info(f\"Rows per second: {stats['rows_per_second']:,.0f}\")\n",
    "        logger.info(f\"Filter efficiency: {stats['filter_efficiency_percent']:.2f}%\")\n",
    "        logger.info(f\"Compression ratio: {stats['compression_ratio']:.2f}x\")\n",
    "\n",
    "        return stats\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     # Initialize processor\n",
    "#     processor = OptimizedParquetBatchProcessor(\n",
    "#         input_file=\"cleaned_ebird_file.pq\",\n",
    "#         output_file=\"filtered_ebird_i95.pq\",\n",
    "#         batch_size=50000,\n",
    "#         distance_threshold=10.0,  # 10 miles\n",
    "#         i95_coords=i95_coords,\n",
    "#         use_compression='snappy'\n",
    "#     )\n",
    "\n",
    "#     # Run the pipeline\n",
    "#     results = processor.run_pipeline()\n",
    "#     print(f\"Processing completed: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-20T21:21:15.366592Z",
     "iopub.status.busy": "2025-06-20T21:21:15.366246Z",
     "iopub.status.idle": "2025-06-20T21:21:28.759801Z",
     "shell.execute_reply": "2025-06-20T21:21:28.759101Z",
     "shell.execute_reply.started": "2025-06-20T21:21:15.366569Z"
    },
    "id": "vAChZiHRVgAH",
    "outputId": "b15d242d-9cdd-4f05-b508-b2787547b340"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 21:21:15,402 - INFO - Parquet file info:\n",
      "2025-06-20 21:21:15,402 - INFO -   Total rows: 23,153\n",
      "2025-06-20 21:21:15,403 - INFO -   Number of row groups: 1\n",
      "2025-06-20 21:21:15,404 - INFO -   Columns: 32\n",
      "2025-06-20 21:21:15,405 - INFO -   File size: 0.00 GB\n",
      "2025-06-20 21:21:15,406 - INFO - Setting up highway geometry with spatial indexing...\n",
      "2025-06-20 21:21:15,473 - INFO - Highway geometry setup complete\n",
      "2025-06-20 21:21:15,474 - INFO - Starting optimized parquet pipeline: temp_birds_input.parquet\n",
      "2025-06-20 21:21:15,474 - INFO - Batch size: 50,000\n",
      "2025-06-20 21:21:15,475 - INFO - Distance threshold: 10.0\n",
      "2025-06-20 21:21:15,475 - INFO - Expected batches: 1\n",
      "2025-06-20 21:21:27,549 - INFO - Saved 23,153 rows to temp_output.parquet\n",
      "2025-06-20 21:21:27,550 - INFO - Optimized parquet pipeline completed!\n",
      "2025-06-20 21:21:27,551 - INFO - Processing time: 0 days 00:00:12.073746\n",
      "2025-06-20 21:21:27,552 - INFO - Rows per second: 1,918\n",
      "2025-06-20 21:21:27,553 - INFO - Filter efficiency: 100.00%\n",
      "2025-06-20 21:21:27,554 - INFO - Compression ratio: 0.00x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed and saved to: s3://birdsbucker/Pipeline_Outputs/processed_birds/state_ebird_i95.parquet\n",
      "Results: {'total_rows_processed': 23153, 'total_rows_saved': 23153, 'total_batches': 1, 'input_file': 'temp_birds_input.parquet', 'output_file': 'temp_output.parquet', 'input_file_size_gb': 0, 'output_file_size_gb': 0.00023735035210847855, 'processing_time_seconds': 12.073746, 'processing_time_formatted': '0 days 00:00:12.073746', 'rows_per_second': 1917.631860070603, 'filter_efficiency_percent': 100.0, 'compression_ratio': 0.0}\n",
      "Cleaned up temporary file: temp_birds_input.parquet\n",
      "Cleaned up temporary file: temp_output.parquet\n"
     ]
    }
   ],
   "source": [
    "### Modify parameters ###\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import s3fs\n",
    "    import boto3\n",
    "    \n",
    "    # S3 configuration\n",
    "    s3_bucket = \"birdsbucker\"\n",
    "    s3_output_path = \"Pipeline_Outputs/processed_birds/state_ebird_i95.parquet\"\n",
    "    s3_output_uri = f\"s3://{s3_bucket}/{s3_output_path}\"\n",
    "    \n",
    "    # Initialize S3 filesystem\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    \n",
    "    # First save the DataFrame to a temporary parquet file\n",
    "    temp_input_file = \"temp_birds_input.parquet\"\n",
    "    temp_output_file = \"temp_output.parquet\"\n",
    "    birds.to_parquet(temp_input_file, compression='snappy')\n",
    "\n",
    "    try:\n",
    "        # Initialize processor with local temporary output\n",
    "        processor = OptimizedParquetBatchProcessor(\n",
    "            input_file=temp_input_file,\n",
    "            output_file=temp_output_file,  # Use local temporary file\n",
    "            batch_size=50000,\n",
    "            distance_threshold=10.0,  # in miles\n",
    "            i95_coords=i95_coords,\n",
    "            use_compression='snappy'\n",
    "        )\n",
    "\n",
    "        # Run the pipeline\n",
    "        results = processor.run_pipeline()\n",
    "        \n",
    "        # Upload the processed file to S3\n",
    "        s3.put(temp_output_file, s3_output_uri)\n",
    "        \n",
    "        print(f\"Processing completed and saved to: {s3_output_uri}\")\n",
    "        print(f\"Results: {results}\")\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        import os\n",
    "        for temp_file in [temp_input_file, temp_output_file]:\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "                print(f\"Cleaned up temporary file: {temp_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2SL0s_N_rdK"
   },
   "outputs": [],
   "source": [
    "# # Read in Ebird data if not too large or loa\n",
    "# birds = pd.read_csv('DATASET', nrows=1000)\n",
    "# print(birds.shape)\n",
    "# print(birds.columns)\n",
    "# print(birds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "my4vUm2uAWIb",
    "outputId": "0fa8b001-46d7-449b-b845-8d09d82e97c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343, 40)\n",
      "Index(['record_type', 'state_code', 'station_id', 'year_record', 'f_system',\n",
      "       'num_lanes', 'sample_type_volume', 'num_lanes_volume', 'method_volume',\n",
      "       'sample_type_class', 'num_lanes_class', 'method_class',\n",
      "       'algorithm_volume', 'num_classes', 'sample_type_truck',\n",
      "       'num_lanes_truck', 'method_truck', 'calibration', 'data_retrieval',\n",
      "       'type_sensor_1', 'type_sensor_2', 'primary_purpose', 'lrs_id',\n",
      "       'lrs_point', 's_lat', 's_lon', 'shrp_id', 'prev_station_id',\n",
      "       'year_established', 'year_discontinued', 'county_code', 'is_sample',\n",
      "       'sample_id', 'nhs', 'posted_route_signing', 'posted_signed_route',\n",
      "       'con_route_signing', 'con_signed_route', 'station_location', 'state'],\n",
      "      dtype='object')\n",
      "  record_type  state_code station_id  year_record f_system  num_lanes  \\\n",
      "0           S          11     001295           23       1U          3   \n",
      "1           S          11     001295           23       1U          3   \n",
      "2           S          11     002295           23       1U          3   \n",
      "3           S          11     S10004           23       1U          2   \n",
      "4           S          11     S10005           23       1U          2   \n",
      "\n",
      "  sample_type_volume  num_lanes_volume  method_volume sample_type_class  ...  \\\n",
      "0                  N                 3              3                 H  ...   \n",
      "1                  T                 3              3                 H  ...   \n",
      "2                  T                 3              3                 H  ...   \n",
      "3                  Y                 2              3                 2  ...   \n",
      "4                  Y                 2              3                 2  ...   \n",
      "\n",
      "   county_code  is_sample     sample_id  nhs posted_route_signing  \\\n",
      "0            0          N          1203    N                    1   \n",
      "1            0          N          1203    N                    1   \n",
      "2            0          N          1203    N                    1   \n",
      "3            0          N          1203    N                    2   \n",
      "4            0          N          1203    N                    2   \n",
      "\n",
      "   posted_signed_route  con_route_signing con_signed_route  \\\n",
      "0             000I-295                  0                0   \n",
      "1                  295                  0                    \n",
      "2                  295                  0                    \n",
      "3                  295                  0                    \n",
      "4                  295                  0                    \n",
      "\n",
      "                                    station_location state  \n",
      "0  I-295@DC Line                                 ...    DC  \n",
      "1  Anacostia North ANN                           ...    DC  \n",
      "2  Anacostia South ANS                           ...    DC  \n",
      "3  I-295 at Chesapeake Street, SW                ...    DC  \n",
      "4  I-295 0.2 Mile North of Chesapeake St, SW     ...    DC  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read in stations data\n",
    "stations = pd.read_csv('/content/drive/MyDrive/Capstone/I95_stations_corrected.csv')\n",
    "print(stations.shape)\n",
    "print(stations.columns)\n",
    "print(stations.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "pwpmidg6t8Ru"
   },
   "outputs": [],
   "source": [
    "def haversine_vectorized(lat1, lon1, lat2, lon2, unit='miles'):\n",
    "    \"\"\"\n",
    "    Vectorized haversine distance calculation (faster than geopy).\n",
    "    \"\"\"\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 3959 if unit == 'miles' else 6371\n",
    "    return c * r\n",
    "\n",
    "def find_nearest_stations_batch(bird_obs, stations, batch_size=5000, debug=False):\n",
    "    \"\"\"\n",
    "    Find nearest station for each bird observation using vectorized Haversine distance.\n",
    "    Handles large datasets efficiently with batching.\n",
    "    \"\"\"\n",
    "    n_obs = len(bird_obs)\n",
    "    nearest_distances = np.full(n_obs, np.inf)\n",
    "    nearest_station_ids = np.full(n_obs, '', dtype=object)\n",
    "\n",
    "    station_coords = stations[['s_lat', 's_lon']].values\n",
    "    station_ids = stations['station_id'].astype(str).values\n",
    "\n",
    "    n_batches = (n_obs + batch_size - 1) // batch_size\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Processing {n_obs} bird observations in {n_batches} batches...\")\n",
    "\n",
    "    for batch_idx in range(n_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, n_obs)\n",
    "        batch_coords = bird_obs.iloc[start_idx:end_idx][['LATITUDE_1', 'LONGITUDE_1']].values\n",
    "\n",
    "        batch_distances = np.array([\n",
    "            haversine_vectorized(b_lat, b_lon, station_coords[:, 0], station_coords[:, 1])\n",
    "            for b_lat, b_lon in batch_coords\n",
    "        ])\n",
    "\n",
    "        nearest_indices = np.argmin(batch_distances, axis=1)\n",
    "        nearest_distances[start_idx:end_idx] = batch_distances[np.arange(len(batch_coords)), nearest_indices]\n",
    "        nearest_station_ids[start_idx:end_idx] = station_ids[nearest_indices]\n",
    "\n",
    "        if debug and batch_idx == 0:\n",
    "            print(\"\\n[DEBUG] Sample nearest station matches (first batch):\")\n",
    "            for i in range(min(3, len(batch_coords))):\n",
    "                print(f\"  Bird {i}:\")\n",
    "                print(f\"    → Nearest station index: {nearest_indices[i]}\")\n",
    "                print(f\"    → Station ID: {station_ids[nearest_indices[i]]}\")\n",
    "                print(f\"    → Distance: {nearest_distances[start_idx + i]:.2f} miles\")\n",
    "\n",
    "    return nearest_distances, nearest_station_ids\n",
    "\n",
    "def add_station_info_to_birds(birds_df, stations_df, bird_lat_col='LATITUDE_1',\n",
    "                              bird_lon_col='LONGITUDE_1', station_lat_col='s_lat',\n",
    "                              station_lon_col='s_lon', batch_size=5000, debug=False):\n",
    "    \"\"\"\n",
    "    Add nearest station information to bird observations using batched spatial distance search.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = birds_df.copy()\n",
    "\n",
    "    if 'station_id' not in stations_df.columns:\n",
    "        stations_df = stations_df.reset_index(drop=True)\n",
    "        stations_df['station_id'] = stations_df.index.astype(str)\n",
    "        if debug:\n",
    "            print(f\"Created station_id column with {len(stations_df)} stations\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Stations DataFrame columns: {list(stations_df.columns)}\")\n",
    "        print(f\"Sample station_ids: {stations_df['station_id'].head().tolist()}\")\n",
    "\n",
    "    distances, station_ids = find_nearest_stations_batch(\n",
    "        result, stations_df, batch_size=batch_size, debug=debug\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Returned distances shape: {distances.shape}\")\n",
    "        print(f\"Returned station_ids shape: {station_ids.shape}\")\n",
    "        print(f\"Sample station_ids: {station_ids[:5]}\")\n",
    "        print(f\"Station_ids type: {type(station_ids[0])}\")\n",
    "\n",
    "    result['station_distance'] = distances\n",
    "    result['nearest_station_id'] = station_ids\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Result columns after assignment: {list(result.columns)}\")\n",
    "        print(f\"Sample result nearest_station_id: {result['nearest_station_id'].head().tolist()}\")\n",
    "\n",
    "    print(f\"Processed {len(result)} bird observations in {time.time() - start_time:.2f}s\")\n",
    "    print(f\"Avg distance: {result['station_distance'].mean():.2f} miles | \"\n",
    "          f\"Median: {result['station_distance'].median():.2f} | \"\n",
    "          f\"Missing: {result['station_distance'].isna().sum()}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdRI3dsXD2ND"
   },
   "source": [
    "To test: run below cell with 'debug = True'\n",
    "To run on entire dataset: run below cel with 'debug = False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "6xd5n9uboCXl"
   },
   "outputs": [],
   "source": [
    "birds = pd.read_parquet('/content/state_ebird_i95.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_KuPYUmD1PH",
    "outputId": "4e94a4cd-a0ad-4871-a059-001f250b030c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations DataFrame columns: ['record_type', 'state_code', 'station_id', 'year_record', 'f_system', 'num_lanes', 'sample_type_volume', 'num_lanes_volume', 'method_volume', 'sample_type_class', 'num_lanes_class', 'method_class', 'algorithm_volume', 'num_classes', 'sample_type_truck', 'num_lanes_truck', 'method_truck', 'calibration', 'data_retrieval', 'type_sensor_1', 'type_sensor_2', 'primary_purpose', 'lrs_id', 'lrs_point', 's_lat', 's_lon', 'shrp_id', 'prev_station_id', 'year_established', 'year_discontinued', 'county_code', 'is_sample', 'sample_id', 'nhs', 'posted_route_signing', 'posted_signed_route', 'con_route_signing', 'con_signed_route', 'station_location', 'state']\n",
      "Sample station_ids: ['001295', '001295', '002295', 'S10004', 'S10005']\n",
      "Processing 23153 bird observations in 3 batches...\n",
      "\n",
      "[DEBUG] Sample nearest station matches (first batch):\n",
      "  Bird 0:\n",
      "    → Nearest station index: 102\n",
      "    → Station ID: 060164\n",
      "    → Distance: 3.17 miles\n",
      "  Bird 1:\n",
      "    → Nearest station index: 102\n",
      "    → Station ID: 060164\n",
      "    → Distance: 3.17 miles\n",
      "  Bird 2:\n",
      "    → Nearest station index: 102\n",
      "    → Station ID: 060164\n",
      "    → Distance: 3.17 miles\n",
      "Returned distances shape: (23153,)\n",
      "Returned station_ids shape: (23153,)\n",
      "Sample station_ids: ['060164' '060164' '060164' '060164' '060164']\n",
      "Station_ids type: <class 'str'>\n",
      "Result columns after assignment: ['state_code', 'station_id', 'year_record', 'month_record', 'day_record', 'day_of_week', 'latitude_0', 'longitude_0', 'station_location', 'state', 'daily_avg_noise', 'peak_hour_noise', 'overnight_noise', 'rush_hour_noise', 'total_daily_volume', 'OBSERVATION COUNT', 'STATE CODE', 'LOCALITY TYPE', 'LATITUDE_1', 'LONGITUDE_1', 'OBSERVATION DATE', 'OBSERVATION DATE_year', 'OBSERVATION DATE_month', 'OBSERVATION DATE_day', 'TimeObservationStarted_hour', 'TimeObservationStarted_minute', 'distance_to_station_160005', 'distance_to_station_140004', 'distance_to_station_160308', 'distance_to_station_060170', 'distance_to_station_792625', 'assigned_station', 'i95_distance', 'station_distance', 'nearest_station_id']\n",
      "Sample result nearest_station_id: ['060164', '060164', '060164', '060164', '060164']\n",
      "Processed 23153 bird observations in 1.00s\n",
      "Avg distance: 3.24 miles | Median: 3.17 | Missing: 0\n"
     ]
    }
   ],
   "source": [
    "### for small dataset Run this cell to add station_distance and station_id to birds dataframe\n",
    "result_df = add_station_info_to_birds(\n",
    "    birds_df = birds,\n",
    "    stations_df=stations,\n",
    "    batch_size=10000,\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhroJItDzXy5"
   },
   "source": [
    "Checking the result_df.  When using without the wrapper, new file does not get created.  Do not forget to **save the result_df** in local drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95DimJnWoOuZ",
    "outputId": "6115465c-f9cd-4780-e592-8ee2e1cb47f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23153, 35)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2ADFBnLxYAh",
    "outputId": "0264fd9e-4ab4-47b0-9b62-6d4fe313b0b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['state_code', 'station_id', 'year_record', 'month_record', 'day_record',\n",
       "       'day_of_week', 'latitude_0', 'longitude_0', 'station_location', 'state',\n",
       "       'daily_avg_noise', 'peak_hour_noise', 'overnight_noise',\n",
       "       'rush_hour_noise', 'total_daily_volume', 'OBSERVATION COUNT',\n",
       "       'STATE CODE', 'LOCALITY TYPE', 'LATITUDE_1', 'LONGITUDE_1',\n",
       "       'OBSERVATION DATE', 'OBSERVATION DATE_year', 'OBSERVATION DATE_month',\n",
       "       'OBSERVATION DATE_day', 'TimeObservationStarted_hour',\n",
       "       'TimeObservationStarted_minute', 'distance_to_station_160005',\n",
       "       'distance_to_station_140004', 'distance_to_station_160308',\n",
       "       'distance_to_station_060170', 'distance_to_station_792625',\n",
       "       'assigned_station', 'i95_distance', 'station_distance',\n",
       "       'nearest_station_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsZFTX15zLDe"
   },
   "source": [
    "A wrapper function for use with the large dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "nsdUAcKNHh9M"
   },
   "outputs": [],
   "source": [
    "# Wrapper function for larger datasets\n",
    "def process_large_dataset_with_stations(\n",
    "    birds_file_path,\n",
    "    stations_df,\n",
    "    output_file_path,\n",
    "    chunk_size=100000,\n",
    "    batch_size=5000,\n",
    "    bird_lat_col='LATITUDE_1',\n",
    "    bird_lon_col='LONGITUDE_1',\n",
    "    station_lat_col='s_lat',\n",
    "    station_lon_col='s_lon',\n",
    "    file_format='parquet',\n",
    "    debug=False,\n",
    "    save_every_n_chunks=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a large bird observations dataset (35M+ rows) with station matching.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    birds_file_path : str\n",
    "        Path to the large bird observations file (CSV, Parquet, etc.)\n",
    "    stations_df : pd.DataFrame\n",
    "        DataFrame containing weather station data\n",
    "    output_file_path : str\n",
    "        Path where the processed results will be saved\n",
    "    chunk_size : int, default=100000\n",
    "        Number of rows to process in each chunk (adjust based on available RAM)\n",
    "    batch_size : int, default=5000\n",
    "        Batch size for the station matching algorithm\n",
    "    bird_lat_col, bird_lon_col : str, optional\n",
    "        Column names for bird coordinates (auto-detected if None)\n",
    "    station_lat_col, station_lon_col : str\n",
    "        Column names for station coordinates\n",
    "    file_format : str, default='parquet'\n",
    "        Output format ('parquet', 'csv', 'feather')\n",
    "    debug : bool, default=False\n",
    "        Enable debug output\n",
    "    save_every_n_chunks : int, default=10\n",
    "        Save intermediate results every N chunks (for crash recovery)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Processing statistics and metadata\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Validate file format\n",
    "    valid_formats = ['parquet', 'csv', 'feather']\n",
    "    if file_format not in valid_formats:\n",
    "        raise ValueError(f\"file_format must be one of {valid_formats}\")\n",
    "\n",
    "    # Determine file reader based on input file extension\n",
    "    file_path = Path(birds_file_path)\n",
    "    if file_path.suffix.lower() == '.csv':\n",
    "        reader_func = pd.read_csv\n",
    "        reader_kwargs = {'chunksize': chunk_size}\n",
    "    elif file_path.suffix.lower() in ['.parquet', '.pq']:\n",
    "        # For parquet, we'll use a different approach since it doesn't have native chunking\n",
    "        reader_func = None\n",
    "        reader_kwargs = {}\n",
    "    else:\n",
    "        # Default to CSV\n",
    "        reader_func = pd.read_csv\n",
    "        reader_kwargs = {'chunksize': chunk_size}\n",
    "\n",
    "    print(f\"Starting processing of large dataset: {birds_file_path}\")\n",
    "    print(f\"Chunk size: {chunk_size:,} | Batch size: {batch_size:,}\")\n",
    "    print(f\"Output format: {file_format} | Save every: {save_every_n_chunks} chunks\")\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    total_rows_processed = 0\n",
    "    chunk_count = 0\n",
    "    processed_chunks = []\n",
    "    temp_files = []\n",
    "\n",
    "    # Create temporary directory for intermediate files\n",
    "    temp_dir = Path(output_file_path).parent / f\"temp_{int(time.time())}\"\n",
    "    temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Handle parquet files differently (read in chunks manually)\n",
    "        if file_path.suffix.lower() in ['.parquet', '.pq']:\n",
    "            # Read parquet file info first\n",
    "            parquet_file = pd.read_parquet(birds_file_path, engine='pyarrow')\n",
    "            total_rows = len(parquet_file)\n",
    "\n",
    "            # Process parquet in manual chunks\n",
    "            for start_row in range(0, total_rows, chunk_size):\n",
    "                end_row = min(start_row + chunk_size, total_rows)\n",
    "                chunk_df = parquet_file.iloc[start_row:end_row].copy()\n",
    "\n",
    "                chunk_count += 1\n",
    "                chunk_start_time = time.time()\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"\\nProcessing chunk {chunk_count} (rows {start_row:,} to {end_row:,})\")\n",
    "\n",
    "                # Process chunk with your existing function\n",
    "                processed_chunk = add_station_info_to_birds(\n",
    "                    chunk_df,\n",
    "                    stations_df,\n",
    "                    bird_lat_col=bird_lat_col,\n",
    "                    bird_lon_col=bird_lon_col,\n",
    "                    station_lat_col=station_lat_col,\n",
    "                    station_lon_col=station_lon_col,\n",
    "                    batch_size=batch_size,\n",
    "                    debug=debug\n",
    "                )\n",
    "\n",
    "                # Save chunk temporarily\n",
    "                temp_file = temp_dir / f\"chunk_{chunk_count:04d}.parquet\"\n",
    "                processed_chunk.to_parquet(temp_file, index=False)\n",
    "                temp_files.append(temp_file)\n",
    "\n",
    "                total_rows_processed += len(processed_chunk)\n",
    "                chunk_time = time.time() - chunk_start_time\n",
    "\n",
    "                print(f\"Chunk {chunk_count} completed in {chunk_time:.2f}s \"\n",
    "                      f\"({len(processed_chunk):,} rows) | \"\n",
    "                      f\"Total: {total_rows_processed:,}/{total_rows:,} \"\n",
    "                      f\"({total_rows_processed/total_rows*100:.1f}%)\")\n",
    "\n",
    "                # Memory management\n",
    "                del chunk_df, processed_chunk\n",
    "                gc.collect()\n",
    "\n",
    "                # Save intermediate results periodically\n",
    "                if chunk_count % save_every_n_chunks == 0:\n",
    "                    print(f\"Saving intermediate results after {chunk_count} chunks...\")\n",
    "                    _save_intermediate_results(temp_files, output_file_path, file_format)\n",
    "\n",
    "            # Clean up the full parquet file from memory\n",
    "            del parquet_file\n",
    "            gc.collect()\n",
    "\n",
    "        else:\n",
    "            # Handle CSV and other formats with chunking\n",
    "            chunk_iterator = reader_func(birds_file_path, **reader_kwargs)\n",
    "\n",
    "            for chunk_df in chunk_iterator:\n",
    "                chunk_count += 1\n",
    "                chunk_start_time = time.time()\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"\\nProcessing chunk {chunk_count} ({len(chunk_df):,} rows)\")\n",
    "\n",
    "                # Process chunk with your existing function\n",
    "                processed_chunk = add_station_info_to_birds(\n",
    "                    chunk_df,\n",
    "                    stations_df,\n",
    "                    bird_lat_col=bird_lat_col,\n",
    "                    bird_lon_col=bird_lon_col,\n",
    "                    station_lat_col=station_lat_col,\n",
    "                    station_lon_col=station_lon_col,\n",
    "                    batch_size=batch_size,\n",
    "                    debug=debug\n",
    "                )\n",
    "\n",
    "                # Save chunk temporarily\n",
    "                temp_file = temp_dir / f\"chunk_{chunk_count:04d}.parquet\"\n",
    "                processed_chunk.to_parquet(temp_file, index=False)\n",
    "                temp_files.append(temp_file)\n",
    "\n",
    "                total_rows_processed += len(processed_chunk)\n",
    "                chunk_time = time.time() - chunk_start_time\n",
    "\n",
    "                print(f\"Chunk {chunk_count} completed in {chunk_time:.2f}s \"\n",
    "                      f\"({len(processed_chunk):,} rows) | Total: {total_rows_processed:,}\")\n",
    "\n",
    "                # Memory management\n",
    "                del chunk_df, processed_chunk\n",
    "                gc.collect()\n",
    "\n",
    "                # Save intermediate results periodically\n",
    "                if chunk_count % save_every_n_chunks == 0:\n",
    "                    print(f\" Saving intermediate results after {chunk_count} chunks...\")\n",
    "                    _save_intermediate_results(temp_files, output_file_path, file_format)\n",
    "\n",
    "        # Final consolidation of all chunks\n",
    "        print(f\"\\nConsolidating {len(temp_files)} chunks into final output...\")\n",
    "        final_df = _consolidate_chunks(temp_files)\n",
    "\n",
    "        # Save final result\n",
    "        _save_final_result(final_df, output_file_path, file_format)\n",
    "\n",
    "        # Calculate statistics\n",
    "        total_time = time.time() - start_time\n",
    "        avg_distance = final_df['station_distance'].mean()\n",
    "        median_distance = final_df['station_distance'].median()\n",
    "        missing_count = final_df['station_distance'].isna().sum()\n",
    "\n",
    "        # Cleanup temporary files\n",
    "        _cleanup_temp_files(temp_dir, temp_files)\n",
    "\n",
    "        stats = {\n",
    "            'total_rows_processed': total_rows_processed,\n",
    "            'total_chunks': chunk_count,\n",
    "            'processing_time_seconds': total_time,\n",
    "            'processing_time_formatted': f\"{total_time/60:.1f} minutes\",\n",
    "            'rows_per_second': total_rows_processed / total_time,\n",
    "            'avg_distance_miles': avg_distance,\n",
    "            'median_distance_miles': median_distance,\n",
    "            'missing_stations': missing_count,\n",
    "            'output_file': output_file_path,\n",
    "            'output_format': file_format\n",
    "        }\n",
    "\n",
    "        print(f\"\\nProcessing completed successfully!\")\n",
    "        print(f\"Total rows: {total_rows_processed:,}\")\n",
    "        print(f\"Total time: {stats['processing_time_formatted']}\")\n",
    "        print(f\"Speed: {stats['rows_per_second']:,.0f} rows/second\")\n",
    "        print(f\"Avg distance: {avg_distance:.2f} miles | Median: {median_distance:.2f} miles\")\n",
    "        print(f\"Missing stations: {missing_count:,} ({missing_count/total_rows_processed*100:.1f}%)\")\n",
    "        print(f\"Output saved to: {output_file_path}\")\n",
    "\n",
    "        return stats\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "        print(f\"Temporary files saved in: {temp_dir}\")\n",
    "        print(f\"You can recover partial results from these files if needed.\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def _save_intermediate_results(temp_files, output_path, file_format):\n",
    "    \"\"\"Save intermediate consolidated results.\"\"\"\n",
    "    if not temp_files:\n",
    "        return\n",
    "\n",
    "    intermediate_df = _consolidate_chunks(temp_files)\n",
    "    intermediate_path = Path(output_path).with_suffix(f'.intermediate.{file_format}')\n",
    "    _save_final_result(intermediate_df, str(intermediate_path), file_format)\n",
    "    print(f\"Intermediate results saved to: {intermediate_path}\")\n",
    "\n",
    "\n",
    "def _consolidate_chunks(temp_files):\n",
    "    \"\"\"Consolidate all temporary chunk files into a single DataFrame.\"\"\"\n",
    "    if not temp_files:\n",
    "        raise ValueError(\"No temporary files to consolidate\")\n",
    "\n",
    "    print(f\"Reading {len(temp_files)} chunk files...\")\n",
    "    chunks = []\n",
    "    for temp_file in temp_files:\n",
    "        chunk = pd.read_parquet(temp_file)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    print(f\"Concatenating chunks...\")\n",
    "    final_df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "    # Clean up memory\n",
    "    del chunks\n",
    "    gc.collect()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def _save_final_result(df, output_path, file_format):\n",
    "    \"\"\"Save the final result in the specified format.\"\"\"\n",
    "    if file_format == 'parquet':\n",
    "        df.to_parquet(output_path, index=False, engine='pyarrow')\n",
    "    elif file_format == 'csv':\n",
    "        df.to_csv(output_path, index=False)\n",
    "    elif file_format == 'feather':\n",
    "        df.to_feather(output_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "\n",
    "\n",
    "def _cleanup_temp_files(temp_dir, temp_files):\n",
    "    \"\"\"Clean up temporary files and directory.\"\"\"\n",
    "    try:\n",
    "        for temp_file in temp_files:\n",
    "            if temp_file.exists():\n",
    "                temp_file.unlink()\n",
    "\n",
    "        if temp_dir.exists():\n",
    "            temp_dir.rmdir()\n",
    "\n",
    "        print(f\"Cleaned up temporary files\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not clean up all temporary files: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUVYyMn4DQbX",
    "outputId": "2dad8b38-a402-4f04-8ff2-1fec6ffba53a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing of large dataset: /content/state_ebird_i95.pq\n",
      "Chunk size: 50,000 | Batch size: 5,000\n",
      "Output format: parquet | Save every: 20 chunks\n",
      "Processed 23153 bird observations in 0.99s\n",
      "Avg distance: 3.24 miles | Median: 3.17 | Missing: 0\n",
      "Chunk 1 completed in 1.03s (23,153 rows) | Total: 23,153/23,153 (100.0%)\n",
      "\n",
      "Consolidating 1 chunks into final output...\n",
      "Reading 1 chunk files...\n",
      "Concatenating chunks...\n",
      "Cleaned up temporary files\n",
      "\n",
      "Processing completed successfully!\n",
      "Total rows: 23,153\n",
      "Total time: 0.0 minutes\n",
      "Speed: 14,526 rows/second\n",
      "Avg distance: 3.24 miles | Median: 3.17 miles\n",
      "Missing stations: 0 (0.0%)\n",
      "Output saved to: ebirds_i95_stations.parquet\n"
     ]
    }
   ],
   "source": [
    "### Run below cell to add station data ###\n",
    "new_birds_file = process_large_dataset_with_stations(\n",
    "    birds_file_path='/content/state_ebird_i95.pq',\n",
    "    stations_df=stations,\n",
    "    output_file_path='ebirds_i95_stations.parquet',\n",
    "    chunk_size=50000,  # Adjust based on your RAM (smaller if you have less RAM)\n",
    "    batch_size=5000,   # Your existing batch size\n",
    "    file_format='parquet',  # Parquet is more efficient for large datasets\n",
    "    debug=False,  # Set to True for detailed output\n",
    "    save_every_n_chunks=20  # Save backup every 20 chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwRg2Nfiyz2_"
   },
   "source": [
    "checking the batch result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "v5zmrDeMmJtF"
   },
   "outputs": [],
   "source": [
    "vs2=pd.read_parquet(\"/content/ebirds_i95_stations.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JVPLCnaKmhiE",
    "outputId": "749cd766-35e7-4f7d-d139-5216c0059e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23153, 35)\n",
      "Index(['state_code', 'station_id', 'year_record', 'month_record', 'day_record',\n",
      "       'day_of_week', 'latitude_0', 'longitude_0', 'station_location', 'state',\n",
      "       'daily_avg_noise', 'peak_hour_noise', 'overnight_noise',\n",
      "       'rush_hour_noise', 'total_daily_volume', 'OBSERVATION COUNT',\n",
      "       'STATE CODE', 'LOCALITY TYPE', 'LATITUDE_1', 'LONGITUDE_1',\n",
      "       'OBSERVATION DATE', 'OBSERVATION DATE_year', 'OBSERVATION DATE_month',\n",
      "       'OBSERVATION DATE_day', 'TimeObservationStarted_hour',\n",
      "       'TimeObservationStarted_minute', 'distance_to_station_160005',\n",
      "       'distance_to_station_140004', 'distance_to_station_160308',\n",
      "       'distance_to_station_060170', 'distance_to_station_792625',\n",
      "       'assigned_station', 'i95_distance', 'station_distance',\n",
      "       'nearest_station_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(vs2.shape)\n",
    "print(vs2.columns)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
