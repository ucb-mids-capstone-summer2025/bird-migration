{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Add 'i95_distance', 'station_distance' and 'station_id\" to Ebird datasets\n",
        "- To used with datasets pre-cleaned using AWS SageMaker Canvas"
      ],
      "metadata": {
        "id": "H89_dLkT7r7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfzBXsw08fR5",
        "outputId": "2ab254e0-aa3b-40f9-8181-329c715bdb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # required installs\n",
        "# !pip install geopandas shapely\n",
        "# !pip install --upgrade pandas numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "4iCuMSW38zpo",
        "outputId": "aeb98458-0fd9-4449-a660-7427f8562756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.0.2)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas) (0.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas) (24.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.2.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyogrio>=0.7.2->geopandas) (2025.6.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, pandas\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.0 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.0 pandas-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              },
              "id": "e9549ee642454fd4a33e6408c5ff914b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# required imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import zipfile\n",
        "import os\n",
        "import logging\n",
        "import time\n",
        "import gc\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from shapely.geometry import Point, LineString\n",
        "from shapely.ops import nearest_points\n",
        "from typing import Optional\n",
        "from geopy.distance import geodesic\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "QpRlqTOZ8uqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "scipy library requires an older numpy version. Unfortunately, using the old numpy inhibits the use of pandas.  Run the following only if calculating"
      ],
      "metadata": {
        "id": "zulQPkeK9TsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Run this first and restart the notebook - resolves scipy-numpy discrepency ###\n",
        "# !pip uninstall -y numpy scipy\n",
        "# !pip install numpy==1.24.4 scipy==1.10.1\n",
        "\n",
        "# # Running this cell will require you to restart the session."
      ],
      "metadata": {
        "id": "o7-aC51496IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # imports for\n",
        "# from scipy.spatial.distance import cdist"
      ],
      "metadata": {
        "id": "RQFgE-K49LoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset check"
      ],
      "metadata": {
        "id": "2wCseRnL8Zfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data read\n",
        "vs = pd.read_parquet(\"/content/VA_test.parquet\")"
      ],
      "metadata": {
        "id": "SZfoRji48pc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUqu30aX7vTf",
        "outputId": "f89fdc97-8efa-455c-8008-022998b13ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23153, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rNZMavo7rXW",
        "outputId": "4f5c3ff2-36b9-4112-c4b0-4c98f3b805f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['state_code', 'station_id', 'year_record', 'month_record', 'day_record',\n",
              "       'day_of_week', 'latitude_0', 'longitude_0', 'station_location', 'state',\n",
              "       'daily_avg_noise', 'peak_hour_noise', 'overnight_noise',\n",
              "       'rush_hour_noise', 'total_daily_volume', 'OBSERVATION COUNT',\n",
              "       'STATE CODE', 'LOCALITY TYPE', 'LATITUDE_1', 'LONGITUDE_1',\n",
              "       'OBSERVATION DATE', 'OBSERVATION DATE_year', 'OBSERVATION DATE_month',\n",
              "       'OBSERVATION DATE_day', 'TimeObservationStarted_hour',\n",
              "       'TimeObservationStarted_minute', 'distance_to_station_160005',\n",
              "       'distance_to_station_140004', 'distance_to_station_160308',\n",
              "       'distance_to_station_060170', 'distance_to_station_792625',\n",
              "       'assigned_station'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "vs.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vs.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEno9w_VXWDN",
        "outputId": "021c9ba4-5453-4205-83b0-4a1e3a89cdeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   state_code  station_id  year_record  month_record  day_record  day_of_week  \\\n",
            "0          51      160005         2023             6          30            6   \n",
            "1          51      160005         2023             6          29            5   \n",
            "2          51      160005         2023             6          28            4   \n",
            "3          51      160005         2023             6          27            3   \n",
            "4          51      160005         2023             6          26            2   \n",
            "\n",
            "   latitude_0  longitude_0                                   station_location  \\\n",
            "0    38.13873    -77.50837  5.85 S RAMP FR RT 1                           ...   \n",
            "1    38.13873    -77.50837  5.85 S RAMP FR RT 1                           ...   \n",
            "2    38.13873    -77.50837  5.85 S RAMP FR RT 1                           ...   \n",
            "3    38.13873    -77.50837  5.85 S RAMP FR RT 1                           ...   \n",
            "4    38.13873    -77.50837  5.85 S RAMP FR RT 1                           ...   \n",
            "\n",
            "  state  ...  OBSERVATION DATE_month  OBSERVATION DATE_day  \\\n",
            "0    VA  ...                       0                    19   \n",
            "1    VA  ...                       0                    19   \n",
            "2    VA  ...                       0                    19   \n",
            "3    VA  ...                       0                    19   \n",
            "4    VA  ...                       0                    19   \n",
            "\n",
            "   TimeObservationStarted_hour  TimeObservationStarted_minute  \\\n",
            "0                           14                              0   \n",
            "1                           14                              0   \n",
            "2                           14                              0   \n",
            "3                           14                              0   \n",
            "4                           14                              0   \n",
            "\n",
            "   distance_to_station_160005  distance_to_station_140004  \\\n",
            "0                    5.323592                   40.842887   \n",
            "1                    5.323592                   40.842887   \n",
            "2                    5.323592                   40.842887   \n",
            "3                    5.323592                   40.842887   \n",
            "4                    5.323592                   40.842887   \n",
            "\n",
            "  distance_to_station_160308 distance_to_station_060170  \\\n",
            "0                  33.085488                   7.701909   \n",
            "1                  33.085488                   7.701909   \n",
            "2                  33.085488                   7.701909   \n",
            "3                  33.085488                   7.701909   \n",
            "4                  33.085488                   7.701909   \n",
            "\n",
            "   distance_to_station_792625  assigned_station  \n",
            "0                   22.943019            160005  \n",
            "1                   22.943019            160005  \n",
            "2                   22.943019            160005  \n",
            "3                   22.943019            160005  \n",
            "4                   22.943019            160005  \n",
            "\n",
            "[5 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vs.LATITUDE_1.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxN96SvFaOMw",
        "outputId": "08fac314-aabf-4bf8-f509-ccba8fee8f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "RangeIndex: 23153 entries, 0 to 23152\n",
            "Series name: LATITUDE_1\n",
            "Non-Null Count  Dtype  \n",
            "--------------  -----  \n",
            "23153 non-null  float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 181.0 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculates distance between bird observation point to I95 - nearest point on a line between the two nearest I95 geo coordinates.\n",
        "Adds 'i95_distance' to Ebirds dataset."
      ],
      "metadata": {
        "id": "tyYUyv1RK_Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Read in Ebird data if not too large or loa\n",
        "# birds = pd.read_csv('DATASET', nrows=1000)\n",
        "# print(birds.shape)\n",
        "# print(birds.columns)\n",
        "# print(birds.head())"
      ],
      "metadata": {
        "id": "DqpRWrJO_g6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i95_coordinates = pd.read_csv('./drive/MyDrive/Capstone/i95_modified.csv')\n",
        "print(i95_coordinates.shape)\n",
        "print(i95_coordinates.columns)\n",
        "print(i95_coordinates.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Elwm3ZhyMYe1",
        "outputId": "88728cf8-bdc0-4db3-db9e-7bf2485de6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(55654, 10)\n",
            "Index(['Way_ID', 'Segment_Number', 'Point_Order', 'Longitude', 'Latitude',\n",
            "       'Highway_Type', 'Route_Ref', 'Max_Speed', 'Combined_Path',\n",
            "       'Overall_Sequence'],\n",
            "      dtype='object')\n",
            "     Way_ID  Segment_Number  Point_Order  Longitude   Latitude Highway_Type  \\\n",
            "0  way/2059               0            0 -77.451046  37.680565     motorway   \n",
            "1  way/2059               0            1 -77.451240  37.681075     motorway   \n",
            "2  way/2059               0            2 -77.451470  37.681770     motorway   \n",
            "3  way/2059               0            3 -77.451679  37.682531     motorway   \n",
            "4  way/2059               0            4 -77.451867  37.683562     motorway   \n",
            "\n",
            "  Route_Ref Max_Speed Combined_Path  Overall_Sequence  \n",
            "0      I 95    65 mph           0_0                 0  \n",
            "1      I 95    65 mph           0_1                 1  \n",
            "2      I 95    65 mph           0_2                 2  \n",
            "3      I 95    65 mph           0_3                 3  \n",
            "4      I 95    65 mph           0_4                 4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i95_sorted = i95_coordinates.sort_values(['Overall_Sequence'])\n",
        "i95_coords = list(zip(i95_sorted['Latitude'], i95_sorted['Longitude']))"
      ],
      "metadata": {
        "id": "Pk81Vwh6OddC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import logging\n",
        "# import pandas as pd\n",
        "# import geopandas as gpd\n",
        "# import numpy as np\n",
        "# from shapely.geometry import Point, LineString\n",
        "# import pyarrow as pa\n",
        "# import pyarrow.parquet as pq\n",
        "# from pathlib import Path\n",
        "# import gc\n",
        "# from typing import Optional, List, Dict, Any\n",
        "\n",
        "### Modify file names before running ###\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class OptimizedParquetBatchProcessor:\n",
        "    \"\"\"\n",
        "    Heavily optimized processor for large parquet files using GeoPandas, spatial indexing,\n",
        "    and vectorized operations with memory-efficient parquet streaming.\n",
        "    Expected 10-100x performance improvement with better memory management.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_file: str = \"cleaned_ebird_file.pq\",\n",
        "                 output_file: str = \"new_file.pq\",\n",
        "                 batch_size: int = 50000,\n",
        "                 distance_threshold: Optional[float] = None,\n",
        "                 i95_coords: Optional[List] = None,\n",
        "                #  columns_to_keep: Optional[List[str]] = None,\n",
        "                 use_compression: str = 'snappy'):\n",
        "\n",
        "        self.input_file = Path(input_file)\n",
        "        self.output_file = Path(output_file)\n",
        "        self.batch_size = batch_size\n",
        "        self.distance_threshold = distance_threshold\n",
        "        # self.columns_to_keep = columns_to_keep\n",
        "        self.use_compression = use_compression\n",
        "\n",
        "        # Validate input file\n",
        "        if not self.input_file.exists():\n",
        "            raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
        "\n",
        "        # Setup parquet file metadata\n",
        "        self._setup_parquet_metadata()\n",
        "\n",
        "        # Pre-process I-95 coordinates into optimized spatial structures\n",
        "        self._setup_highway_geometry(i95_coords)\n",
        "\n",
        "        # Statistics tracking\n",
        "        self.total_rows_processed = 0\n",
        "        self.total_rows_saved = 0\n",
        "        self.batch_count = 0\n",
        "        self.total_file_size = 0\n",
        "\n",
        "    def _setup_parquet_metadata(self):\n",
        "        \"\"\"Get parquet file metadata for optimization\"\"\"\n",
        "        try:\n",
        "            # Read parquet metadata\n",
        "            parquet_file = pq.ParquetFile(self.input_file)\n",
        "            self.parquet_metadata = parquet_file.metadata\n",
        "            self.parquet_schema = parquet_file.schema\n",
        "            self.total_rows = self.parquet_metadata.num_rows\n",
        "\n",
        "            logger.info(f\"Parquet file info:\")\n",
        "            logger.info(f\"  Total rows: {self.total_rows:,}\")\n",
        "            logger.info(f\"  Number of row groups: {self.parquet_metadata.num_row_groups}\")\n",
        "            logger.info(f\"  Columns: {len(self.parquet_schema)}\")\n",
        "\n",
        "            # Get file size\n",
        "            self.total_file_size = self.input_file.stat().st_size / (1024**3)  # GB\n",
        "            logger.info(f\"  File size: {self.total_file_size:.2f} GB\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading parquet metadata: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _setup_highway_geometry(self, i95_coords):\n",
        "        \"\"\"Convert I-95 coordinates to optimized spatial structures\"\"\"\n",
        "        if not i95_coords:\n",
        "            raise ValueError(\"I-95 coordinates must be provided\")\n",
        "\n",
        "        logger.info(\"Setting up highway geometry with spatial indexing...\")\n",
        "\n",
        "        # Create LineString geometry from coordinates\n",
        "        # i95_coords are (lat,lon) but LineString expects (lon,lat), so swap them\n",
        "        self.highway_line = LineString([(lon, lat) for lat, lon in i95_coords])\n",
        "\n",
        "        # Create GeoDataFrame for the highway with spatial index\n",
        "        highway_gdf = gpd.GeoDataFrame([1], geometry=[self.highway_line], crs='EPSG:4326')\n",
        "\n",
        "        # Convert to projected CRS for accurate distance calculations (UTM Zone 18N)\n",
        "        self.highway_gdf_projected = highway_gdf.to_crs('EPSG:32618')\n",
        "        self.highway_line_projected = self.highway_gdf_projected.geometry.iloc[0]\n",
        "\n",
        "        logger.info(\"Highway geometry setup complete\")\n",
        "\n",
        "    def calculate_distances_vectorized(self, obs_gdf: gpd.GeoDataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Vectorized distance calculation using GeoPandas\n",
        "        This is the key optimization - processes all points at once\n",
        "        \"\"\"\n",
        "        # Project observations to same CRS as highway (UTM Zone 18N)\n",
        "        obs_projected = obs_gdf.to_crs('EPSG:32618')\n",
        "\n",
        "        # Vectorized distance calculation to highway line\n",
        "        distances_meters = obs_projected.geometry.distance(self.highway_line_projected)\n",
        "\n",
        "        # Convert meters to miles\n",
        "        distances_miles = distances_meters * 0.000621371\n",
        "\n",
        "        return distances_miles.values\n",
        "\n",
        "    def process_batch_optimized(self, batch_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Optimized batch processing using vectorized operations\"\"\"\n",
        "\n",
        "        # Early return if empty batch\n",
        "        if batch_df.empty:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Filter out rows with invalid coordinates early\n",
        "        valid_coords = batch_df.dropna(subset=['LATITUDE_1', 'LONGITUDE_1'])\n",
        "\n",
        "        if len(valid_coords) == 0:\n",
        "            logger.warning(\"No valid coordinates in batch\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Convert to numeric and filter realistic coordinate ranges\n",
        "        valid_coords = valid_coords.copy()\n",
        "\n",
        "        # Use more efficient numeric conversion\n",
        "        coord_cols = ['LATITUDE_1', 'LONGITUDE_1']\n",
        "        for col in coord_cols:\n",
        "            if valid_coords[col].dtype == 'object':\n",
        "                valid_coords[col] = pd.to_numeric(valid_coords[col], errors='coerce')\n",
        "\n",
        "        # Filter to reasonable coordinate bounds (roughly continental US)\n",
        "        coord_filter = (\n",
        "            (valid_coords['LATITUDE_1'].between(24, 50)) &\n",
        "            (valid_coords['LONGITUDE_1'].between(-130, -65))\n",
        "        )\n",
        "        valid_coords = valid_coords[coord_filter]\n",
        "\n",
        "        if len(valid_coords) == 0:\n",
        "            logger.warning(\"No valid coordinates after filtering\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Create GeoDataFrame from observations\n",
        "        geometry = gpd.points_from_xy(valid_coords['LONGITUDE_1'], valid_coords['LATITUDE_1'])\n",
        "        obs_gdf = gpd.GeoDataFrame(valid_coords, geometry=geometry, crs='EPSG:4326')\n",
        "\n",
        "        # Calculate distances using vectorized operation\n",
        "        distances = self.calculate_distances_vectorized(obs_gdf)\n",
        "\n",
        "        # Add distances to dataframe\n",
        "        result_df = valid_coords.copy()\n",
        "        result_df['i95_distance'] = distances\n",
        "\n",
        "        # Apply distance filter if specified\n",
        "        if self.distance_threshold is not None:\n",
        "            result_df = result_df[result_df['i95_distance'] <= self.distance_threshold]\n",
        "\n",
        "        # # Select only specified columns if provided\n",
        "        # if self.columns_to_keep:\n",
        "        #     available_cols = [col for col in self.columns_to_keep + ['i95_distance']\n",
        "        #                     if col in result_df.columns]\n",
        "        #     result_df = result_df[available_cols]\n",
        "\n",
        "        return result_df\n",
        "\n",
        "    def _process_one_batch(self, batch_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process a single batch with logging and memory management\"\"\"\n",
        "        self.batch_count += 1\n",
        "        self.total_rows_processed += len(batch_df)\n",
        "\n",
        "        if self.batch_count % 10 == 0:  # Log every 10th batch to reduce noise\n",
        "            progress = (self.total_rows_processed / self.total_rows) * 100\n",
        "            logger.info(f\"Processing batch {self.batch_count} ({progress:.1f}% complete) \"\n",
        "                       f\"with {len(batch_df)} rows\")\n",
        "\n",
        "        processed_batch = self.process_batch_optimized(batch_df)\n",
        "        self.total_rows_saved += len(processed_batch)\n",
        "\n",
        "        # Force garbage collection periodically\n",
        "        if self.batch_count % 50 == 0:\n",
        "            gc.collect()\n",
        "\n",
        "        return processed_batch\n",
        "\n",
        "    def _read_parquet_in_batches(self):\n",
        "        \"\"\"Generator to read parquet file in batches efficiently\"\"\"\n",
        "        try:\n",
        "            # Use pyarrow for more efficient reading\n",
        "            parquet_file = pq.ParquetFile(self.input_file)\n",
        "\n",
        "            # Read in batches using row groups when possible\n",
        "            for batch in parquet_file.iter_batches(batch_size=self.batch_size):\n",
        "                # Convert to pandas DataFrame\n",
        "                df = batch.to_pandas()\n",
        "                yield df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading parquet in batches: {e}\")\n",
        "            # Fallback to pandas chunking\n",
        "            logger.info(\"Falling back to pandas chunking...\")\n",
        "            try:\n",
        "                # Read the entire file and chunk it manually since pandas doesn't support chunksize for parquet\n",
        "                df = pd.read_parquet(self.input_file)\n",
        "                for i in range(0, len(df), self.batch_size):\n",
        "                    yield df.iloc[i:i+self.batch_size]\n",
        "                del df  # Free memory\n",
        "            except Exception as e2:\n",
        "                logger.error(f\"Fallback also failed: {e2}\")\n",
        "                raise\n",
        "\n",
        "    def _write_parquet_batch(self, df: pd.DataFrame, is_first_batch: bool = False):\n",
        "        \"\"\"Write batch to parquet file efficiently\"\"\"\n",
        "        if df.empty:\n",
        "            return\n",
        "\n",
        "        # Convert to Arrow Table for efficient writing\n",
        "        table = pa.Table.from_pandas(df)\n",
        "\n",
        "        if is_first_batch:\n",
        "            # Create new file with compatible parameters\n",
        "            writer = pq.ParquetWriter(\n",
        "                self.output_file,\n",
        "                table.schema,\n",
        "                compression=self.use_compression,\n",
        "                use_dictionary=True  # Enable dictionary encoding\n",
        "            )\n",
        "            self._parquet_writer = writer\n",
        "\n",
        "        # Write the batch\n",
        "        self._parquet_writer.write_table(table)\n",
        "\n",
        "    def run_pipeline(self) -> Dict[str, Any]:\n",
        "        \"\"\"Run the complete optimized pipeline for parquet files\"\"\"\n",
        "        logger.info(f\"Starting optimized parquet pipeline: {self.input_file}\")\n",
        "        logger.info(f\"Batch size: {self.batch_size:,}\")\n",
        "        logger.info(f\"Distance threshold: {self.distance_threshold}\")\n",
        "        logger.info(f\"Expected batches: {(self.total_rows // self.batch_size) + 1}\")\n",
        "\n",
        "        start_time = pd.Timestamp.now()\n",
        "        first_batch = True\n",
        "\n",
        "        try:\n",
        "            # Process file in batches\n",
        "            for batch_df in self._read_parquet_in_batches():\n",
        "                processed_batch = self._process_one_batch(batch_df)\n",
        "\n",
        "                if not processed_batch.empty:\n",
        "                    self._write_parquet_batch(processed_batch, is_first_batch=first_batch)\n",
        "                    first_batch = False\n",
        "\n",
        "            # Close the parquet writer\n",
        "            if hasattr(self, '_parquet_writer'):\n",
        "                self._parquet_writer.close()\n",
        "                logger.info(f\"Saved {self.total_rows_saved:,} rows to {self.output_file}\")\n",
        "            else:\n",
        "                logger.warning(\"No data to save\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Pipeline error: {str(e)}\")\n",
        "            # Clean up partial file\n",
        "            if self.output_file.exists():\n",
        "                self.output_file.unlink()\n",
        "            raise\n",
        "\n",
        "        finally:\n",
        "            # Cleanup\n",
        "            if hasattr(self, '_parquet_writer'):\n",
        "                try:\n",
        "                    self._parquet_writer.close()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        end_time = pd.Timestamp.now()\n",
        "        processing_time = (end_time - start_time).total_seconds()\n",
        "\n",
        "        # Calculate statistics\n",
        "        stats = {\n",
        "            'total_rows_processed': self.total_rows_processed,\n",
        "            'total_rows_saved': self.total_rows_saved,\n",
        "            'total_batches': self.batch_count,\n",
        "            'input_file': str(self.input_file),\n",
        "            'output_file': str(self.output_file),\n",
        "            'input_file_size_gb': self.total_file_size,\n",
        "            'output_file_size_gb': self.output_file.stat().st_size / (1024**3) if self.output_file.exists() else 0,\n",
        "            'processing_time_seconds': processing_time,\n",
        "            'processing_time_formatted': str(pd.Timedelta(seconds=processing_time)),\n",
        "            'rows_per_second': self.total_rows_processed / processing_time if processing_time > 0 else 0,\n",
        "            'filter_efficiency_percent': (self.total_rows_saved / self.total_rows_processed * 100)\n",
        "                                        if self.total_rows_processed > 0 else 0,\n",
        "            'compression_ratio': (self.total_file_size / (self.output_file.stat().st_size / (1024**3)))\n",
        "                               if self.output_file.exists() and self.output_file.stat().st_size > 0 else 0\n",
        "        }\n",
        "\n",
        "        logger.info(\"Optimized parquet pipeline completed!\")\n",
        "        logger.info(f\"Processing time: {stats['processing_time_formatted']}\")\n",
        "        logger.info(f\"Rows per second: {stats['rows_per_second']:,.0f}\")\n",
        "        logger.info(f\"Filter efficiency: {stats['filter_efficiency_percent']:.2f}%\")\n",
        "        logger.info(f\"Compression ratio: {stats['compression_ratio']:.2f}x\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "# # Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     # Initialize processor\n",
        "#     processor = OptimizedParquetBatchProcessor(\n",
        "#         input_file=\"cleaned_ebird_file.pq\",\n",
        "#         output_file=\"filtered_ebird_i95.pq\",\n",
        "#         batch_size=50000,\n",
        "#         distance_threshold=10.0,  # 10 miles\n",
        "#         i95_coords=i95_coords,\n",
        "#         use_compression='snappy'\n",
        "#     )\n",
        "\n",
        "#     # Run the pipeline\n",
        "#     results = processor.run_pipeline()\n",
        "#     print(f\"Processing completed: {results}\")"
      ],
      "metadata": {
        "id": "0UlyE5h4c5kL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Modify parameters ###\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Initialize processor\n",
        "    processor = OptimizedParquetBatchProcessor(\n",
        "        input_file=\"VA_test.parquet\",\n",
        "        output_file=\"state_ebird_i95.pq\",\n",
        "        batch_size=50000,\n",
        "        distance_threshold=10.0,  # in miles\n",
        "        i95_coords=i95_coords,\n",
        "        use_compression='snappy'\n",
        "    )\n",
        "\n",
        "    # Run the pipeline\n",
        "    results = processor.run_pipeline()\n",
        "    print(f\"Processing completed: {results}\")"
      ],
      "metadata": {
        "id": "vAChZiHRVgAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b15d242d-9cdd-4f05-b508-b2787547b340"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed: {'total_rows_processed': 23153, 'total_rows_saved': 23153, 'total_batches': 1, 'input_file': 'VA_test.parquet', 'output_file': 'state_ebird_i95.pq', 'input_file_size_gb': 0, 'output_file_size_gb': 0.00023735687136650085, 'processing_time_seconds': 15.117894, 'processing_time_formatted': '0 days 00:00:15.117894', 'rows_per_second': 1531.496384350889, 'filter_efficiency_percent': 100.0, 'compression_ratio': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vs1 = pd.read_parquet(\"/content/state_ebird_i95.pq\")\n",
        "print(vs1.shape)\n",
        "print(vs1.columns)"
      ],
      "metadata": {
        "id": "yAwrBL7TRBW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a439bf2-595b-418c-e609-370b56506f7e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23153, 33)\n",
            "Index(['state_code', 'station_id', 'year_record', 'month_record', 'day_record',\n",
            "       'day_of_week', 'latitude_0', 'longitude_0', 'station_location', 'state',\n",
            "       'daily_avg_noise', 'peak_hour_noise', 'overnight_noise',\n",
            "       'rush_hour_noise', 'total_daily_volume', 'OBSERVATION COUNT',\n",
            "       'STATE CODE', 'LOCALITY TYPE', 'LATITUDE_1', 'LONGITUDE_1',\n",
            "       'OBSERVATION DATE', 'OBSERVATION DATE_year', 'OBSERVATION DATE_month',\n",
            "       'OBSERVATION DATE_day', 'TimeObservationStarted_hour',\n",
            "       'TimeObservationStarted_minute', 'distance_to_station_160005',\n",
            "       'distance_to_station_140004', 'distance_to_station_160308',\n",
            "       'distance_to_station_060170', 'distance_to_station_792625',\n",
            "       'assigned_station', 'i95_distance'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vs1.i95_distance.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "PhmJ7dOUgN6F",
        "outputId": "e2242ed9-3a9a-440e-e995-295fd3563a13"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    23153.000000\n",
              "mean         0.141768\n",
              "std          0.177003\n",
              "min          0.002472\n",
              "25%          0.035271\n",
              "50%          0.066202\n",
              "75%          0.189376\n",
              "max          0.853026\n",
              "Name: i95_distance, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>i95_distance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>23153.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.141768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.177003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.002472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.035271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.066202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.189376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.853026</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate distance between bird observation point to the nearest noise measure station\n",
        "- coded to process in batches to accomodate the large Ebird data set.\n",
        "- use the corrected stations data with geo coordinates: 'I95_stations_corrected.csv':\n",
        " - 'latitude' & 'longitude' columns were renamed to 's_lat' & 's_lon'\n",
        " - 'longitude' values corrected to U.S. coordinates<br>\n",
        "\n",
        "Adds 'station_distance' & 'station_id' to Ebirds dataset."
      ],
      "metadata": {
        "id": "4YIGWadK_R5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Read in Ebird data if not too large or loa\n",
        "# birds = pd.read_csv('DATASET', nrows=1000)\n",
        "# print(birds.shape)\n",
        "# print(birds.columns)\n",
        "# print(birds.head())"
      ],
      "metadata": {
        "id": "v2SL0s_N_rdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in stations data\n",
        "stations = pd.read_csv('/content/drive/MyDrive/Capstone/I95_stations_corrected.csv')\n",
        "print(stations.shape)\n",
        "print(stations.columns)\n",
        "print(stations.head())"
      ],
      "metadata": {
        "id": "my4vUm2uAWIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fa8b001-46d7-449b-b845-8d09d82e97c9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(343, 40)\n",
            "Index(['record_type', 'state_code', 'station_id', 'year_record', 'f_system',\n",
            "       'num_lanes', 'sample_type_volume', 'num_lanes_volume', 'method_volume',\n",
            "       'sample_type_class', 'num_lanes_class', 'method_class',\n",
            "       'algorithm_volume', 'num_classes', 'sample_type_truck',\n",
            "       'num_lanes_truck', 'method_truck', 'calibration', 'data_retrieval',\n",
            "       'type_sensor_1', 'type_sensor_2', 'primary_purpose', 'lrs_id',\n",
            "       'lrs_point', 's_lat', 's_lon', 'shrp_id', 'prev_station_id',\n",
            "       'year_established', 'year_discontinued', 'county_code', 'is_sample',\n",
            "       'sample_id', 'nhs', 'posted_route_signing', 'posted_signed_route',\n",
            "       'con_route_signing', 'con_signed_route', 'station_location', 'state'],\n",
            "      dtype='object')\n",
            "  record_type  state_code station_id  year_record f_system  num_lanes  \\\n",
            "0           S          11     001295           23       1U          3   \n",
            "1           S          11     001295           23       1U          3   \n",
            "2           S          11     002295           23       1U          3   \n",
            "3           S          11     S10004           23       1U          2   \n",
            "4           S          11     S10005           23       1U          2   \n",
            "\n",
            "  sample_type_volume  num_lanes_volume  method_volume sample_type_class  ...  \\\n",
            "0                  N                 3              3                 H  ...   \n",
            "1                  T                 3              3                 H  ...   \n",
            "2                  T                 3              3                 H  ...   \n",
            "3                  Y                 2              3                 2  ...   \n",
            "4                  Y                 2              3                 2  ...   \n",
            "\n",
            "   county_code  is_sample     sample_id  nhs posted_route_signing  \\\n",
            "0            0          N          1203    N                    1   \n",
            "1            0          N          1203    N                    1   \n",
            "2            0          N          1203    N                    1   \n",
            "3            0          N          1203    N                    2   \n",
            "4            0          N          1203    N                    2   \n",
            "\n",
            "   posted_signed_route  con_route_signing con_signed_route  \\\n",
            "0             000I-295                  0                0   \n",
            "1                  295                  0                    \n",
            "2                  295                  0                    \n",
            "3                  295                  0                    \n",
            "4                  295                  0                    \n",
            "\n",
            "                                    station_location state  \n",
            "0  I-295@DC Line                                 ...    DC  \n",
            "1  Anacostia North ANN                           ...    DC  \n",
            "2  Anacostia South ANS                           ...    DC  \n",
            "3  I-295 at Chesapeake Street, SW                ...    DC  \n",
            "4  I-295 0.2 Mile North of Chesapeake St, SW     ...    DC  \n",
            "\n",
            "[5 rows x 40 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def haversine_vectorized(lat1, lon1, lat2, lon2, unit='miles'):\n",
        "    \"\"\"\n",
        "    Vectorized haversine distance calculation (faster than geopy).\n",
        "    \"\"\"\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    r = 3959 if unit == 'miles' else 6371\n",
        "    return c * r\n",
        "\n",
        "def find_nearest_stations_batch(bird_obs, stations, batch_size=5000, debug=False):\n",
        "    \"\"\"\n",
        "    Find nearest station for each bird observation using vectorized Haversine distance.\n",
        "    Handles large datasets efficiently with batching.\n",
        "    \"\"\"\n",
        "    n_obs = len(bird_obs)\n",
        "    nearest_distances = np.full(n_obs, np.inf)\n",
        "    nearest_station_ids = np.full(n_obs, '', dtype=object)\n",
        "\n",
        "    station_coords = stations[['s_lat', 's_lon']].values\n",
        "    station_ids = stations['station_id'].astype(str).values\n",
        "\n",
        "    n_batches = (n_obs + batch_size - 1) // batch_size\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Processing {n_obs} bird observations in {n_batches} batches...\")\n",
        "\n",
        "    for batch_idx in range(n_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min((batch_idx + 1) * batch_size, n_obs)\n",
        "        batch_coords = bird_obs.iloc[start_idx:end_idx][['LATITUDE_1', 'LONGITUDE_1']].values\n",
        "\n",
        "        batch_distances = np.array([\n",
        "            haversine_vectorized(b_lat, b_lon, station_coords[:, 0], station_coords[:, 1])\n",
        "            for b_lat, b_lon in batch_coords\n",
        "        ])\n",
        "\n",
        "        nearest_indices = np.argmin(batch_distances, axis=1)\n",
        "        nearest_distances[start_idx:end_idx] = batch_distances[np.arange(len(batch_coords)), nearest_indices]\n",
        "        nearest_station_ids[start_idx:end_idx] = station_ids[nearest_indices]\n",
        "\n",
        "        if debug and batch_idx == 0:\n",
        "            print(\"\\n[DEBUG] Sample nearest station matches (first batch):\")\n",
        "            for i in range(min(3, len(batch_coords))):\n",
        "                print(f\"  Bird {i}:\")\n",
        "                print(f\"    → Nearest station index: {nearest_indices[i]}\")\n",
        "                print(f\"    → Station ID: {station_ids[nearest_indices[i]]}\")\n",
        "                print(f\"    → Distance: {nearest_distances[start_idx + i]:.2f} miles\")\n",
        "\n",
        "    return nearest_distances, nearest_station_ids\n",
        "\n",
        "def add_station_info_to_birds(birds_df, stations_df, bird_lat_col='LATITUDE_1',\n",
        "                              bird_lon_col='LONGITUDE_1', station_lat_col='s_lat',\n",
        "                              station_lon_col='s_lon', batch_size=5000, debug=False):\n",
        "    \"\"\"\n",
        "    Add nearest station information to bird observations using batched spatial distance search.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    result = birds_df.copy()\n",
        "\n",
        "    if 'station_id' not in stations_df.columns:\n",
        "        stations_df = stations_df.reset_index(drop=True)\n",
        "        stations_df['station_id'] = stations_df.index.astype(str)\n",
        "        if debug:\n",
        "            print(f\"Created station_id column with {len(stations_df)} stations\")\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Stations DataFrame columns: {list(stations_df.columns)}\")\n",
        "        print(f\"Sample station_ids: {stations_df['station_id'].head().tolist()}\")\n",
        "\n",
        "    distances, station_ids = find_nearest_stations_batch(\n",
        "        result, stations_df, batch_size=batch_size, debug=debug\n",
        "    )\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Returned distances shape: {distances.shape}\")\n",
        "        print(f\"Returned station_ids shape: {station_ids.shape}\")\n",
        "        print(f\"Sample station_ids: {station_ids[:5]}\")\n",
        "        print(f\"Station_ids type: {type(station_ids[0])}\")\n",
        "\n",
        "    result['station_distance'] = distances\n",
        "    result['nearest_station_id'] = station_ids\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Result columns after assignment: {list(result.columns)}\")\n",
        "        print(f\"Sample result nearest_station_id: {result['nearest_station_id'].head().tolist()}\")\n",
        "\n",
        "    print(f\"Processed {len(result)} bird observations in {time.time() - start_time:.2f}s\")\n",
        "    print(f\"Avg distance: {result['station_distance'].mean():.2f} miles | \"\n",
        "          f\"Median: {result['station_distance'].median():.2f} | \"\n",
        "          f\"Missing: {result['station_distance'].isna().sum()}\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "pwpmidg6t8Ru"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test: run below cell with 'debug = True'\n",
        "To run on entire dataset: run below cel with 'debug = False'"
      ],
      "metadata": {
        "id": "PdRI3dsXD2ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "birds = pd.read_parquet('/content/state_ebird_i95.pq')"
      ],
      "metadata": {
        "id": "6xd5n9uboCXl"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### for small dataset Run this cell to add station_distance and station_id to birds dataframe\n",
        "result_df = add_station_info_to_birds(\n",
        "    birds_df = birds,\n",
        "    stations_df=stations,\n",
        "    batch_size=10000,\n",
        "    debug=True\n",
        ")"
      ],
      "metadata": {
        "id": "B_KuPYUmD1PH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e94a4cd-a0ad-4871-a059-001f250b030c"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stations DataFrame columns: ['record_type', 'state_code', 'station_id', 'year_record', 'f_system', 'num_lanes', 'sample_type_volume', 'num_lanes_volume', 'method_volume', 'sample_type_class', 'num_lanes_class', 'method_class', 'algorithm_volume', 'num_classes', 'sample_type_truck', 'num_lanes_truck', 'method_truck', 'calibration', 'data_retrieval', 'type_sensor_1', 'type_sensor_2', 'primary_purpose', 'lrs_id', 'lrs_point', 's_lat', 's_lon', 'shrp_id', 'prev_station_id', 'year_established', 'year_discontinued', 'county_code', 'is_sample', 'sample_id', 'nhs', 'posted_route_signing', 'posted_signed_route', 'con_route_signing', 'con_signed_route', 'station_location', 'state']\n",
            "Sample station_ids: ['001295', '001295', '002295', 'S10004', 'S10005']\n",
            "Processing 23153 bird observations in 3 batches...\n",
            "\n",
            "[DEBUG] Sample nearest station matches (first batch):\n",
            "  Bird 0:\n",
            "    → Nearest station index: 102\n",
            "    → Station ID: 060164\n",
            "    → Distance: 3.17 miles\n",
            "  Bird 1:\n",
            "    → Nearest station index: 102\n",
            "    → Station ID: 060164\n",
            "    → Distance: 3.17 miles\n",
            "  Bird 2:\n",
            "    → Nearest station index: 102\n",
            "    → Station ID: 060164\n",
            "    → Distance: 3.17 miles\n",
            "Returned distances shape: (23153,)\n",
            "Returned station_ids shape: (23153,)\n",
            "Sample station_ids: ['060164' '060164' '060164' '060164' '060164']\n",
            "Station_ids type: <class 'str'>\n",
            "Result columns after assignment: ['state_code', 'station_id', 'year_record', 'month_record', 'day_record', 'day_of_week', 'latitude_0', 'longitude_0', 'station_location', 'state', 'daily_avg_noise', 'peak_hour_noise', 'overnight_noise', 'rush_hour_noise', 'total_daily_volume', 'OBSERVATION COUNT', 'STATE CODE', 'LOCALITY TYPE', 'LATITUDE_1', 'LONGITUDE_1', 'OBSERVATION DATE', 'OBSERVATION DATE_year', 'OBSERVATION DATE_month', 'OBSERVATION DATE_day', 'TimeObservationStarted_hour', 'TimeObservationStarted_minute', 'distance_to_station_160005', 'distance_to_station_140004', 'distance_to_station_160308', 'distance_to_station_060170', 'distance_to_station_792625', 'assigned_station', 'i95_distance', 'station_distance', 'nearest_station_id']\n",
            "Sample result nearest_station_id: ['060164', '060164', '060164', '060164', '060164']\n",
            "Processed 23153 bird observations in 1.00s\n",
            "Avg distance: 3.24 miles | Median: 3.17 | Missing: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the result_df.  When using without the wrapper, new file does not get created.  Do not forget to **save the result_df** in local drive."
      ],
      "metadata": {
        "id": "GhroJItDzXy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95DimJnWoOuZ",
        "outputId": "6115465c-f9cd-4780-e592-8ee2e1cb47f7"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23153, 35)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2ADFBnLxYAh",
        "outputId": "0264fd9e-4ab4-47b0-9b62-6d4fe313b0b0"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['state_code', 'station_id', 'year_record', 'month_record', 'day_record',\n",
              "       'day_of_week', 'latitude_0', 'longitude_0', 'station_location', 'state',\n",
              "       'daily_avg_noise', 'peak_hour_noise', 'overnight_noise',\n",
              "       'rush_hour_noise', 'total_daily_volume', 'OBSERVATION COUNT',\n",
              "       'STATE CODE', 'LOCALITY TYPE', 'LATITUDE_1', 'LONGITUDE_1',\n",
              "       'OBSERVATION DATE', 'OBSERVATION DATE_year', 'OBSERVATION DATE_month',\n",
              "       'OBSERVATION DATE_day', 'TimeObservationStarted_hour',\n",
              "       'TimeObservationStarted_minute', 'distance_to_station_160005',\n",
              "       'distance_to_station_140004', 'distance_to_station_160308',\n",
              "       'distance_to_station_060170', 'distance_to_station_792625',\n",
              "       'assigned_station', 'i95_distance', 'station_distance',\n",
              "       'nearest_station_id'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A wrapper function for use with the large dataset.   "
      ],
      "metadata": {
        "id": "lsZFTX15zLDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper function for larger datasets\n",
        "def process_large_dataset_with_stations(\n",
        "    birds_file_path,\n",
        "    stations_df,\n",
        "    output_file_path,\n",
        "    chunk_size=100000,\n",
        "    batch_size=5000,\n",
        "    bird_lat_col='LATITUDE_1',\n",
        "    bird_lon_col='LONGITUDE_1',\n",
        "    station_lat_col='s_lat',\n",
        "    station_lon_col='s_lon',\n",
        "    file_format='parquet',\n",
        "    debug=False,\n",
        "    save_every_n_chunks=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Process a large bird observations dataset (35M+ rows) with station matching.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    birds_file_path : str\n",
        "        Path to the large bird observations file (CSV, Parquet, etc.)\n",
        "    stations_df : pd.DataFrame\n",
        "        DataFrame containing weather station data\n",
        "    output_file_path : str\n",
        "        Path where the processed results will be saved\n",
        "    chunk_size : int, default=100000\n",
        "        Number of rows to process in each chunk (adjust based on available RAM)\n",
        "    batch_size : int, default=5000\n",
        "        Batch size for the station matching algorithm\n",
        "    bird_lat_col, bird_lon_col : str, optional\n",
        "        Column names for bird coordinates (auto-detected if None)\n",
        "    station_lat_col, station_lon_col : str\n",
        "        Column names for station coordinates\n",
        "    file_format : str, default='parquet'\n",
        "        Output format ('parquet', 'csv', 'feather')\n",
        "    debug : bool, default=False\n",
        "        Enable debug output\n",
        "    save_every_n_chunks : int, default=10\n",
        "        Save intermediate results every N chunks (for crash recovery)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Processing statistics and metadata\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Validate file format\n",
        "    valid_formats = ['parquet', 'csv', 'feather']\n",
        "    if file_format not in valid_formats:\n",
        "        raise ValueError(f\"file_format must be one of {valid_formats}\")\n",
        "\n",
        "    # Determine file reader based on input file extension\n",
        "    file_path = Path(birds_file_path)\n",
        "    if file_path.suffix.lower() == '.csv':\n",
        "        reader_func = pd.read_csv\n",
        "        reader_kwargs = {'chunksize': chunk_size}\n",
        "    elif file_path.suffix.lower() in ['.parquet', '.pq']:\n",
        "        # For parquet, we'll use a different approach since it doesn't have native chunking\n",
        "        reader_func = None\n",
        "        reader_kwargs = {}\n",
        "    else:\n",
        "        # Default to CSV\n",
        "        reader_func = pd.read_csv\n",
        "        reader_kwargs = {'chunksize': chunk_size}\n",
        "\n",
        "    print(f\"Starting processing of large dataset: {birds_file_path}\")\n",
        "    print(f\"Chunk size: {chunk_size:,} | Batch size: {batch_size:,}\")\n",
        "    print(f\"Output format: {file_format} | Save every: {save_every_n_chunks} chunks\")\n",
        "\n",
        "    # Initialize tracking variables\n",
        "    total_rows_processed = 0\n",
        "    chunk_count = 0\n",
        "    processed_chunks = []\n",
        "    temp_files = []\n",
        "\n",
        "    # Create temporary directory for intermediate files\n",
        "    temp_dir = Path(output_file_path).parent / f\"temp_{int(time.time())}\"\n",
        "    temp_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Handle parquet files differently (read in chunks manually)\n",
        "        if file_path.suffix.lower() in ['.parquet', '.pq']:\n",
        "            # Read parquet file info first\n",
        "            parquet_file = pd.read_parquet(birds_file_path, engine='pyarrow')\n",
        "            total_rows = len(parquet_file)\n",
        "\n",
        "            # Process parquet in manual chunks\n",
        "            for start_row in range(0, total_rows, chunk_size):\n",
        "                end_row = min(start_row + chunk_size, total_rows)\n",
        "                chunk_df = parquet_file.iloc[start_row:end_row].copy()\n",
        "\n",
        "                chunk_count += 1\n",
        "                chunk_start_time = time.time()\n",
        "\n",
        "                if debug:\n",
        "                    print(f\"\\nProcessing chunk {chunk_count} (rows {start_row:,} to {end_row:,})\")\n",
        "\n",
        "                # Process chunk with your existing function\n",
        "                processed_chunk = add_station_info_to_birds(\n",
        "                    chunk_df,\n",
        "                    stations_df,\n",
        "                    bird_lat_col=bird_lat_col,\n",
        "                    bird_lon_col=bird_lon_col,\n",
        "                    station_lat_col=station_lat_col,\n",
        "                    station_lon_col=station_lon_col,\n",
        "                    batch_size=batch_size,\n",
        "                    debug=debug\n",
        "                )\n",
        "\n",
        "                # Save chunk temporarily\n",
        "                temp_file = temp_dir / f\"chunk_{chunk_count:04d}.parquet\"\n",
        "                processed_chunk.to_parquet(temp_file, index=False)\n",
        "                temp_files.append(temp_file)\n",
        "\n",
        "                total_rows_processed += len(processed_chunk)\n",
        "                chunk_time = time.time() - chunk_start_time\n",
        "\n",
        "                print(f\"Chunk {chunk_count} completed in {chunk_time:.2f}s \"\n",
        "                      f\"({len(processed_chunk):,} rows) | \"\n",
        "                      f\"Total: {total_rows_processed:,}/{total_rows:,} \"\n",
        "                      f\"({total_rows_processed/total_rows*100:.1f}%)\")\n",
        "\n",
        "                # Memory management\n",
        "                del chunk_df, processed_chunk\n",
        "                gc.collect()\n",
        "\n",
        "                # Save intermediate results periodically\n",
        "                if chunk_count % save_every_n_chunks == 0:\n",
        "                    print(f\"Saving intermediate results after {chunk_count} chunks...\")\n",
        "                    _save_intermediate_results(temp_files, output_file_path, file_format)\n",
        "\n",
        "            # Clean up the full parquet file from memory\n",
        "            del parquet_file\n",
        "            gc.collect()\n",
        "\n",
        "        else:\n",
        "            # Handle CSV and other formats with chunking\n",
        "            chunk_iterator = reader_func(birds_file_path, **reader_kwargs)\n",
        "\n",
        "            for chunk_df in chunk_iterator:\n",
        "                chunk_count += 1\n",
        "                chunk_start_time = time.time()\n",
        "\n",
        "                if debug:\n",
        "                    print(f\"\\nProcessing chunk {chunk_count} ({len(chunk_df):,} rows)\")\n",
        "\n",
        "                # Process chunk with your existing function\n",
        "                processed_chunk = add_station_info_to_birds(\n",
        "                    chunk_df,\n",
        "                    stations_df,\n",
        "                    bird_lat_col=bird_lat_col,\n",
        "                    bird_lon_col=bird_lon_col,\n",
        "                    station_lat_col=station_lat_col,\n",
        "                    station_lon_col=station_lon_col,\n",
        "                    batch_size=batch_size,\n",
        "                    debug=debug\n",
        "                )\n",
        "\n",
        "                # Save chunk temporarily\n",
        "                temp_file = temp_dir / f\"chunk_{chunk_count:04d}.parquet\"\n",
        "                processed_chunk.to_parquet(temp_file, index=False)\n",
        "                temp_files.append(temp_file)\n",
        "\n",
        "                total_rows_processed += len(processed_chunk)\n",
        "                chunk_time = time.time() - chunk_start_time\n",
        "\n",
        "                print(f\"Chunk {chunk_count} completed in {chunk_time:.2f}s \"\n",
        "                      f\"({len(processed_chunk):,} rows) | Total: {total_rows_processed:,}\")\n",
        "\n",
        "                # Memory management\n",
        "                del chunk_df, processed_chunk\n",
        "                gc.collect()\n",
        "\n",
        "                # Save intermediate results periodically\n",
        "                if chunk_count % save_every_n_chunks == 0:\n",
        "                    print(f\" Saving intermediate results after {chunk_count} chunks...\")\n",
        "                    _save_intermediate_results(temp_files, output_file_path, file_format)\n",
        "\n",
        "        # Final consolidation of all chunks\n",
        "        print(f\"\\nConsolidating {len(temp_files)} chunks into final output...\")\n",
        "        final_df = _consolidate_chunks(temp_files)\n",
        "\n",
        "        # Save final result\n",
        "        _save_final_result(final_df, output_file_path, file_format)\n",
        "\n",
        "        # Calculate statistics\n",
        "        total_time = time.time() - start_time\n",
        "        avg_distance = final_df['station_distance'].mean()\n",
        "        median_distance = final_df['station_distance'].median()\n",
        "        missing_count = final_df['station_distance'].isna().sum()\n",
        "\n",
        "        # Cleanup temporary files\n",
        "        _cleanup_temp_files(temp_dir, temp_files)\n",
        "\n",
        "        stats = {\n",
        "            'total_rows_processed': total_rows_processed,\n",
        "            'total_chunks': chunk_count,\n",
        "            'processing_time_seconds': total_time,\n",
        "            'processing_time_formatted': f\"{total_time/60:.1f} minutes\",\n",
        "            'rows_per_second': total_rows_processed / total_time,\n",
        "            'avg_distance_miles': avg_distance,\n",
        "            'median_distance_miles': median_distance,\n",
        "            'missing_stations': missing_count,\n",
        "            'output_file': output_file_path,\n",
        "            'output_format': file_format\n",
        "        }\n",
        "\n",
        "        print(f\"\\nProcessing completed successfully!\")\n",
        "        print(f\"Total rows: {total_rows_processed:,}\")\n",
        "        print(f\"Total time: {stats['processing_time_formatted']}\")\n",
        "        print(f\"Speed: {stats['rows_per_second']:,.0f} rows/second\")\n",
        "        print(f\"Avg distance: {avg_distance:.2f} miles | Median: {median_distance:.2f} miles\")\n",
        "        print(f\"Missing stations: {missing_count:,} ({missing_count/total_rows_processed*100:.1f}%)\")\n",
        "        print(f\"Output saved to: {output_file_path}\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during processing: {str(e)}\")\n",
        "        print(f\"Temporary files saved in: {temp_dir}\")\n",
        "        print(f\"You can recover partial results from these files if needed.\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def _save_intermediate_results(temp_files, output_path, file_format):\n",
        "    \"\"\"Save intermediate consolidated results.\"\"\"\n",
        "    if not temp_files:\n",
        "        return\n",
        "\n",
        "    intermediate_df = _consolidate_chunks(temp_files)\n",
        "    intermediate_path = Path(output_path).with_suffix(f'.intermediate.{file_format}')\n",
        "    _save_final_result(intermediate_df, str(intermediate_path), file_format)\n",
        "    print(f\"Intermediate results saved to: {intermediate_path}\")\n",
        "\n",
        "\n",
        "def _consolidate_chunks(temp_files):\n",
        "    \"\"\"Consolidate all temporary chunk files into a single DataFrame.\"\"\"\n",
        "    if not temp_files:\n",
        "        raise ValueError(\"No temporary files to consolidate\")\n",
        "\n",
        "    print(f\"Reading {len(temp_files)} chunk files...\")\n",
        "    chunks = []\n",
        "    for temp_file in temp_files:\n",
        "        chunk = pd.read_parquet(temp_file)\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    print(f\"Concatenating chunks...\")\n",
        "    final_df = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "    # Clean up memory\n",
        "    del chunks\n",
        "    gc.collect()\n",
        "\n",
        "    return final_df\n",
        "\n",
        "\n",
        "def _save_final_result(df, output_path, file_format):\n",
        "    \"\"\"Save the final result in the specified format.\"\"\"\n",
        "    if file_format == 'parquet':\n",
        "        df.to_parquet(output_path, index=False, engine='pyarrow')\n",
        "    elif file_format == 'csv':\n",
        "        df.to_csv(output_path, index=False)\n",
        "    elif file_format == 'feather':\n",
        "        df.to_feather(output_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
        "\n",
        "\n",
        "def _cleanup_temp_files(temp_dir, temp_files):\n",
        "    \"\"\"Clean up temporary files and directory.\"\"\"\n",
        "    try:\n",
        "        for temp_file in temp_files:\n",
        "            if temp_file.exists():\n",
        "                temp_file.unlink()\n",
        "\n",
        "        if temp_dir.exists():\n",
        "            temp_dir.rmdir()\n",
        "\n",
        "        print(f\"Cleaned up temporary files\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not clean up all temporary files: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nsdUAcKNHh9M"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Run below cell to add station data ###\n",
        "new_birds_file = process_large_dataset_with_stations(\n",
        "    birds_file_path='/content/state_ebird_i95.pq',\n",
        "    stations_df=stations,\n",
        "    output_file_path='ebirds_i95_stations.parquet',\n",
        "    chunk_size=50000,  # Adjust based on your RAM (smaller if you have less RAM)\n",
        "    batch_size=5000,   # Your existing batch size\n",
        "    file_format='parquet',  # Parquet is more efficient for large datasets\n",
        "    debug=False,  # Set to True for detailed output\n",
        "    save_every_n_chunks=20  # Save backup every 20 chunks\n",
        ")"
      ],
      "metadata": {
        "id": "SUVYyMn4DQbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dad8b38-a402-4f04-8ff2-1fec6ffba53a"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting processing of large dataset: /content/state_ebird_i95.pq\n",
            "Chunk size: 50,000 | Batch size: 5,000\n",
            "Output format: parquet | Save every: 20 chunks\n",
            "Processed 23153 bird observations in 0.99s\n",
            "Avg distance: 3.24 miles | Median: 3.17 | Missing: 0\n",
            "Chunk 1 completed in 1.03s (23,153 rows) | Total: 23,153/23,153 (100.0%)\n",
            "\n",
            "Consolidating 1 chunks into final output...\n",
            "Reading 1 chunk files...\n",
            "Concatenating chunks...\n",
            "Cleaned up temporary files\n",
            "\n",
            "Processing completed successfully!\n",
            "Total rows: 23,153\n",
            "Total time: 0.0 minutes\n",
            "Speed: 14,526 rows/second\n",
            "Avg distance: 3.24 miles | Median: 3.17 miles\n",
            "Missing stations: 0 (0.0%)\n",
            "Output saved to: ebirds_i95_stations.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking the batch result"
      ],
      "metadata": {
        "id": "ZwRg2Nfiyz2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vs2=pd.read_parquet(\"/content/ebirds_i95_stations.parquet\")"
      ],
      "metadata": {
        "id": "v5zmrDeMmJtF"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vs2.shape)\n",
        "print(vs2.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVPLCnaKmhiE",
        "outputId": "749cd766-35e7-4f7d-d139-5216c0059e38"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23153, 35)\n",
            "Index(['state_code', 'station_id', 'year_record', 'month_record', 'day_record',\n",
            "       'day_of_week', 'latitude_0', 'longitude_0', 'station_location', 'state',\n",
            "       'daily_avg_noise', 'peak_hour_noise', 'overnight_noise',\n",
            "       'rush_hour_noise', 'total_daily_volume', 'OBSERVATION COUNT',\n",
            "       'STATE CODE', 'LOCALITY TYPE', 'LATITUDE_1', 'LONGITUDE_1',\n",
            "       'OBSERVATION DATE', 'OBSERVATION DATE_year', 'OBSERVATION DATE_month',\n",
            "       'OBSERVATION DATE_day', 'TimeObservationStarted_hour',\n",
            "       'TimeObservationStarted_minute', 'distance_to_station_160005',\n",
            "       'distance_to_station_140004', 'distance_to_station_160308',\n",
            "       'distance_to_station_060170', 'distance_to_station_792625',\n",
            "       'assigned_station', 'i95_distance', 'station_distance',\n",
            "       'nearest_station_id'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    }
  ]
}