{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f8da9a",
   "metadata": {},
   "source": [
    "# Birds Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e1fb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sooneui/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a9fbe",
   "metadata": {},
   "source": [
    "## Initial Filtering & Cleaning of Ebird Data<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9d237",
   "metadata": {},
   "source": [
    "### Initial feature drop <br>\n",
    "- Remove unused columns & prepare state data for transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62084245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xt/_chwp0056v52sycwjk0jctdm0000gn/T/ipykernel_34845/2349757709.py:1: DtypeWarning: Columns (9,21,23,24,33,38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  VA_birds = pd.read_csv(\"/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ebd_US-VA_202001_202312_smp_relMay-2025/ebd_US-VA_202001_202312_smp_relMay-2025.txt\", sep = \"\\t\")\n",
      "/var/folders/xt/_chwp0056v52sycwjk0jctdm0000gn/T/ipykernel_34845/2349757709.py:2: DtypeWarning: Columns (9,21,23,33,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ME_birds = pd.read_csv(\"/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ebd_US-ME_202001_202312_smp_relMay-2025/ebd_US-ME_202001_202312_smp_relMay-2025.txt\", sep = \"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VA_birds: (14875315, 53)\n",
      "ME_birds: (6697506, 53)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VA_birds = pd.read_csv(\"/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ebd_US-VA_202001_202312_smp_relMay-2025/ebd_US-VA_202001_202312_smp_relMay-2025.txt\", sep = \"\\t\")\n",
    "ME_birds = pd.read_csv(\"/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ebd_US-ME_202001_202312_smp_relMay-2025/ebd_US-ME_202001_202312_smp_relMay-2025.txt\", sep = \"\\t\")\n",
    "print(\"VA_birds:\", VA_birds.shape), print(\"ME_birds:\", ME_birds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff9ed1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GLOBAL UNIQUE IDENTIFIER', 'LAST EDITED DATE', 'TAXONOMIC ORDER',\n",
       "       'CATEGORY', 'TAXON CONCEPT ID', 'COMMON NAME', 'SCIENTIFIC NAME',\n",
       "       'SUBSPECIES COMMON NAME', 'SUBSPECIES SCIENTIFIC NAME', 'EXOTIC CODE',\n",
       "       'OBSERVATION COUNT', 'BREEDING CODE', 'BREEDING CATEGORY',\n",
       "       'BEHAVIOR CODE', 'AGE/SEX', 'COUNTRY', 'COUNTRY CODE', 'STATE',\n",
       "       'STATE CODE', 'COUNTY', 'COUNTY CODE', 'IBA CODE', 'BCR CODE',\n",
       "       'USFWS CODE', 'ATLAS BLOCK', 'LOCALITY', 'LOCALITY ID', 'LOCALITY TYPE',\n",
       "       'LATITUDE', 'LONGITUDE', 'OBSERVATION DATE',\n",
       "       'TIME OBSERVATIONS STARTED', 'OBSERVER ID', 'OBSERVER ORCID ID',\n",
       "       'SAMPLING EVENT IDENTIFIER', 'OBSERVATION TYPE', 'PROTOCOL NAME',\n",
       "       'PROTOCOL CODE', 'PROJECT NAMES', 'PROJECT IDENTIFIERS',\n",
       "       'DURATION MINUTES', 'EFFORT DISTANCE KM', 'EFFORT AREA HA',\n",
       "       'NUMBER OBSERVERS', 'ALL SPECIES REPORTED', 'GROUP IDENTIFIER',\n",
       "       'HAS MEDIA', 'APPROVED', 'REVIEWED', 'REASON', 'CHECKLIST COMMENTS',\n",
       "       'SPECIES COMMENTS', 'Unnamed: 52'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VA_birds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e828ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VA: (14875315, 13) ME: (6697506, 13)\n"
     ]
    }
   ],
   "source": [
    "##Selecting relevant columns\n",
    "VA_birds=VA_birds[['TAXON CONCEPT ID', 'COMMON NAME', 'SCIENTIFIC NAME', 'OBSERVATION COUNT',\n",
    "       'STATE CODE','COUNTY', 'LOCALITY', 'LOCALITY TYPE', 'LATITUDE', 'LONGITUDE', \n",
    "       'OBSERVATION DATE', 'TIME OBSERVATIONS STARTED', 'DURATION MINUTES']].copy()\n",
    "ME_birds=ME_birds[['TAXON CONCEPT ID', 'COMMON NAME', 'SCIENTIFIC NAME', 'OBSERVATION COUNT',\n",
    "       'STATE CODE','COUNTY', 'LOCALITY', 'LOCALITY TYPE', 'LATITUDE', 'LONGITUDE', \n",
    "       'OBSERVATION DATE', 'TIME OBSERVATIONS STARTED', 'DURATION MINUTES']].copy()\n",
    "print(\"VA:\", VA_birds.shape, \"ME:\", ME_birds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1e47e",
   "metadata": {},
   "source": [
    "### Filter each state by study area polygons<br>\n",
    "Requires 'ME_polygon.json' and 'VA_polygon.json': designated study area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e46dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to Filter observations within pre-defined polygon\n",
    "def filter_df_within_polygon(df, polygon_geojson, lat_col='LATITUDE', lon_col='LONGITUDE'):\n",
    "    try:\n",
    "        import geopandas as gpd\n",
    "        from shapely.geometry import Point, Polygon\n",
    "        \n",
    "        # Extract polygon coordinates\n",
    "        polygon_coords = polygon_geojson['features'][0]['geometry']['coordinates'][0]\n",
    "        polygon = Polygon(polygon_coords)\n",
    "        \n",
    "        # Create GeoDataFrame - use 'df' parameter, not 'df_clean'\n",
    "        geometry = [Point(lon, lat) for lon, lat in zip(df[lon_col], df[lat_col])]\n",
    "        gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "        \n",
    "        # Filter using spatial operation - removed trailing period\n",
    "        filtered_gdf = gdf[gdf.geometry.within(polygon)]\n",
    "        \n",
    "        # Return as regular DataFrame (drop geometry column)\n",
    "        return filtered_gdf.drop(columns=['geometry'])\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"GeoPandas not installed. Using fallback method.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab1166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VA_polygon = pd.read_json('/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/VA_polygon.json')\n",
    "ME_polygon = pd.read_json('/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/ME_polygon.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b45877b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43098, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter VA by Polygon\n",
    "VA_birds_filtered = filter_df_within_polygon(VA_birds, VA_polygon)\n",
    "VA_birds_filtered.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2f042c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(908976, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter ME by Polygon\n",
    "ME_birds_filtered = filter_df_within_polygon(ME_birds, ME_polygon)\n",
    "ME_birds_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d559606",
   "metadata": {},
   "source": [
    "### Combine data into a single df/file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd0b3d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "952074"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birds=pd.concat([VA_birds_filtered, ME_birds_filtered], axis=0, ignore_index=True)\n",
    "len(birds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360ece0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "birds.to_parquet('/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/stations.parquet', compression='snappy') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31699721",
   "metadata": {},
   "source": [
    "## Add Derived Features and Clean Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6998e",
   "metadata": {},
   "source": [
    "### Add station_distance & assigned_station\n",
    "Adds following features to the df:\n",
    "- station_distance: str listing stations within the defined polygon and the distances to each station from the bird observation coordiante\n",
    "- assigned_station: str containing the nearest noise detection station from the bird observation coordinate<br>\n",
    "\n",
    "Requires following files\n",
    "- \"I95_stations_master\": A file listing all noise detection stations along/near I95 highway is required \n",
    "- \"stations_in_polygon\": A file listing noise detection stations within the study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "180f3839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 6)\n",
      "Index(['station_id', 'longitude', 'latitude', 'state', 'geometry',\n",
      "       'index_right'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "stations_in_polygons = pd.read_parquet(\"/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/stations_in_polygons.parquet\") \n",
    "print(stations_in_polygons.shape)\n",
    "print(stations_in_polygons.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a353f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stations_list\n",
    "stations_list = stations_in_polygons[['station_id', 'latitude', 'longitude']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c25582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate distance between two points using Haversine formula\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Calculate the great-circle distance between two points on the Earth\n",
    "    R = 6371.0  # Earth radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(float, (lat1, lon1, lat2, lon2))\n",
    "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dphi/2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def add_station_distances_and_assignment(pandas_df, stations, lat_col='LATITUDE', lon_col='LONGITUDE', id_col='station_id'):\n",
    "\n",
    "    # stations should be a list of dicts with keys: id_col, latitude, longitude\n",
    "    station_ids = [str(st[id_col]) for st in stations]\n",
    "    station_coords = [(float(st['latitude']), float(st['longitude'])) for st in stations]\n",
    "\n",
    "    # Calculate distances for each row\n",
    "    def calculate_distances(row):\n",
    "        try:\n",
    "            if pd.isna(row[lat_col]) or pd.isna(row[lon_col]):\n",
    "                print(f\"Found NA values: lat={row[lat_col]}, lon={row[lon_col]}\")\n",
    "                return None\n",
    "\n",
    "            distances = {}\n",
    "            for station_id, (st_lat, st_lon) in zip(station_ids, station_coords):\n",
    "                try:\n",
    "                    # Convert to float explicitly and handle any string formatting\n",
    "                    lat1 = float(str(row[lat_col]).strip())\n",
    "                    lon1 = float(str(row[lon_col]).strip())\n",
    "                    dist = haversine(lat1, lon1, st_lat, st_lon)\n",
    "                    distances[station_id] = dist\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating distance for station {station_id}: {str(e)}\")\n",
    "                    print(f\"Values: lat1={row[lat_col]}, lon1={row[lon_col]}, st_lat={st_lat}, st_lon={st_lon}\")\n",
    "                    distances[station_id] = None\n",
    "            return distances\n",
    "        except Exception as e:\n",
    "            print(f\"Error in calculate_distances: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    # Add distances dictionary column\n",
    "    pandas_df = pandas_df.copy()\n",
    "    pandas_df['station_distances'] = pandas_df.apply(calculate_distances, axis=1)\n",
    "\n",
    "    # Find closest station\n",
    "    def assign_station(distances_dict):\n",
    "        if not distances_dict:\n",
    "            return None\n",
    "\n",
    "        min_dist = float('inf')\n",
    "        min_stations = []\n",
    "\n",
    "        for station_id, dist in distances_dict.items():\n",
    "            if dist is None:\n",
    "                continue\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_stations = [station_id]\n",
    "            elif dist == min_dist:\n",
    "                min_stations.append(station_id)\n",
    "\n",
    "        if not min_stations:\n",
    "            return None\n",
    "\n",
    "        # If tie, pick randomly\n",
    "        return random.choice(min_stations)\n",
    "\n",
    "    # Add assigned station column\n",
    "    pandas_df['assigned_station'] = pandas_df['station_distances'].apply(assign_station)\n",
    "\n",
    "    return pandas_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4f2553a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape before adding station distances: (952074, 13)\n",
      "df shape after adding station distances: (952074, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"df shape before adding station distances:\", birds.shape)\n",
    "birds = add_station_distances_and_assignment(birds, stations_list, lat_col='LATITUDE', lon_col='LONGITUDE', id_col='station_id')\n",
    "print(\"df shape after adding station distances:\", birds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c831a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "birds.to_parquet('/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/stations.parquet', compression='snappy')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5e21b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NA in 'station_distances': TAXON CONCEPT ID                 0\n",
      "COMMON NAME                      0\n",
      "SCIENTIFIC NAME                  0\n",
      "OBSERVATION COUNT                0\n",
      "STATE CODE                       0\n",
      "COUNTY                           0\n",
      "LOCALITY                         0\n",
      "LOCALITY TYPE                    0\n",
      "LATITUDE                         0\n",
      "LONGITUDE                        0\n",
      "OBSERVATION DATE                 0\n",
      "TIME OBSERVATIONS STARTED     3321\n",
      "DURATION MINUTES             62586\n",
      "station_distances                0\n",
      "assigned_station                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "na_count = birds.isna().sum()\n",
    "print(f\"Number of rows with NA in 'station_distances': {na_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bbaf2",
   "metadata": {},
   "source": [
    "### Add i95_distances <br>\n",
    "Adds i95_distance feature to the df.<br>\n",
    "Requires \"i95_coordinates.csv\", a file containing i95 coordinates with road sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bfaefd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55654, 10)\n",
      "Index(['Way_ID', 'Segment_Number', 'Point_Order', 'Longitude', 'Latitude',\n",
      "       'Highway_Type', 'Route_Ref', 'Max_Speed', 'Combined_Path',\n",
      "       'Overall_Sequence'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# read in the i95 coordinates\n",
    "# This file contains the coordinates of the I-95 highway for the analysis\n",
    "i95_coordinates = pd.read_csv('/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/i95_coordinates.csv')\n",
    "print(i95_coordinates.shape)\n",
    "print(i95_coordinates.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c665f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "i95_sorted = i95_coordinates.sort_values(['Overall_Sequence'])\n",
    "i95_coords = list(zip(i95_sorted['Latitude'], i95_sorted['Longitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "323429c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I95 DistanceCalculator ###\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class I95DistanceCalculator:\n",
    "    \"\"\"\n",
    "    Simplified processor to add I-95 distance column to parquet files.\n",
    "    Only adds the distance column without any filtering or row removal.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_file: str = \"cleaned_ebird_file.pq\",\n",
    "                 output_file: str = \"new_file.pq\",\n",
    "                 batch_size: int = 50000,\n",
    "                 i95_coords: Optional[List] = i95_coords,\n",
    "                 use_compression: str = 'snappy'):\n",
    "\n",
    "        self.input_file = Path(input_file)\n",
    "        self.output_file = Path(output_file)\n",
    "        self.batch_size = batch_size\n",
    "        self.use_compression = use_compression\n",
    "\n",
    "        # Setup parquet file metadata\n",
    "        self._setup_parquet_metadata()\n",
    "\n",
    "        # Pre-process I-95 coordinates into optimized spatial structures\n",
    "        self._setup_highway_geometry(i95_coords)\n",
    "\n",
    "        # Statistics tracking\n",
    "        self.total_rows_processed = 0\n",
    "        self.batch_count = 0\n",
    "\n",
    "    def _setup_parquet_metadata(self):\n",
    "        \"\"\"Get parquet file metadata for optimization\"\"\"\n",
    "        try:\n",
    "            # Read parquet metadata\n",
    "            parquet_file = pq.ParquetFile(self.input_file)\n",
    "            self.parquet_metadata = parquet_file.metadata\n",
    "            self.parquet_schema = parquet_file.schema\n",
    "            self.total_rows = self.parquet_metadata.num_rows\n",
    "\n",
    "            logger.info(f\"Parquet file info:\")\n",
    "            logger.info(f\"  Total rows: {self.total_rows:,}\")\n",
    "            logger.info(f\"  Number of row groups: {self.parquet_metadata.num_row_groups}\")\n",
    "            logger.info(f\"  Columns: {len(self.parquet_schema)}\")\n",
    "\n",
    "            # Get file size\n",
    "            self.total_file_size = self.input_file.stat().st_size / (1024**3)  # GB\n",
    "            logger.info(f\"  File size: {self.total_file_size:.2f} GB\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading parquet metadata: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_highway_geometry(self, i95_coords):\n",
    "        \"\"\"Convert I-95 coordinates to optimized spatial structures\"\"\"\n",
    "        if not i95_coords:\n",
    "            raise ValueError(\"I-95 coordinates must be provided\")\n",
    "\n",
    "        logger.info(\"Setting up highway geometry with spatial indexing...\")\n",
    "\n",
    "        # Create LineString geometry from coordinates\n",
    "        # i95_coords are (lat,lon) but LineString expects (lon,lat), so swap them\n",
    "        self.highway_line = LineString([(lon, lat) for lat, lon in i95_coords])\n",
    "\n",
    "        # Create GeoDataFrame for the highway with spatial index\n",
    "        highway_gdf = gpd.GeoDataFrame([1], geometry=[self.highway_line], crs='EPSG:4326')\n",
    "\n",
    "        # Convert to projected CRS for accurate distance calculations (UTM Zone 18N)\n",
    "        self.highway_gdf_projected = highway_gdf.to_crs('EPSG:32618')\n",
    "        self.highway_line_projected = self.highway_gdf_projected.geometry.iloc[0]\n",
    "\n",
    "        logger.info(\"Highway geometry setup complete\")\n",
    "\n",
    "    def calculate_distances_vectorized(self, obs_gdf: gpd.GeoDataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Vectorized distance calculation using GeoPandas\n",
    "        This is the key optimization - processes all points at once\n",
    "        \"\"\"\n",
    "        # Project observations to same CRS as highway (UTM Zone 18N)\n",
    "        obs_projected = obs_gdf.to_crs('EPSG:32618')\n",
    "\n",
    "        # Vectorized distance calculation to highway line\n",
    "        distances_meters = obs_projected.geometry.distance(self.highway_line_projected)\n",
    "\n",
    "        # Convert meters to miles\n",
    "        distances_miles = distances_meters * 0.000621371\n",
    "\n",
    "        return distances_miles.values\n",
    "\n",
    "    def process_batch(self, batch_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process batch to add I-95 distance column\"\"\"\n",
    "        \n",
    "        # Start with copy of original batch\n",
    "        result_df = batch_df.copy()\n",
    "        \n",
    "        # Initialize distance column with NaN\n",
    "        result_df['i95_distance'] = np.nan\n",
    "        \n",
    "        # Find rows with valid coordinates\n",
    "        valid_mask = (\n",
    "            batch_df['LATITUDE'].notna() & \n",
    "            batch_df['LONGITUDE'].notna()\n",
    "        )\n",
    "        \n",
    "        if not valid_mask.any():\n",
    "            logger.warning(\"No valid coordinates in batch\")\n",
    "            return result_df\n",
    "        \n",
    "        # Get valid coordinates\n",
    "        valid_coords = batch_df[valid_mask].copy()\n",
    "        \n",
    "        # Convert to numeric if needed\n",
    "        coord_cols = ['LATITUDE', 'LONGITUDE']\n",
    "        for col in coord_cols:\n",
    "            if valid_coords[col].dtype == 'object':\n",
    "                valid_coords[col] = pd.to_numeric(valid_coords[col], errors='coerce')\n",
    "        \n",
    "        # Update mask after numeric conversion\n",
    "        numeric_valid_mask = (\n",
    "            valid_coords['LATITUDE'].notna() & \n",
    "            valid_coords['LONGITUDE'].notna()\n",
    "        )\n",
    "        \n",
    "        if not numeric_valid_mask.any():\n",
    "            logger.warning(\"No valid coordinates after numeric conversion\")\n",
    "            return result_df\n",
    "        \n",
    "        # Get final valid coordinates\n",
    "        final_valid_coords = valid_coords[numeric_valid_mask]\n",
    "        \n",
    "        # Create GeoDataFrame from valid observations\n",
    "        geometry = gpd.points_from_xy(final_valid_coords['LONGITUDE'], final_valid_coords['LATITUDE'])\n",
    "        obs_gdf = gpd.GeoDataFrame(final_valid_coords, geometry=geometry, crs='EPSG:4326')\n",
    "        \n",
    "        # Calculate distances using vectorized operation\n",
    "        distances = self.calculate_distances_vectorized(obs_gdf)\n",
    "        \n",
    "        # Add distances back to result dataframe at correct positions\n",
    "        result_df.loc[final_valid_coords.index, 'i95_distance'] = distances\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def _process_one_batch(self, batch_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process a single batch with logging and memory management\"\"\"\n",
    "        self.batch_count += 1\n",
    "        self.total_rows_processed += len(batch_df)\n",
    "\n",
    "        if self.batch_count % 10 == 0:  # Log every 10th batch to reduce noise\n",
    "            progress = (self.total_rows_processed / self.total_rows) * 100\n",
    "            logger.info(f\"Processing batch {self.batch_count} ({progress:.1f}% complete) \"\n",
    "                       f\"with {len(batch_df)} rows\")\n",
    "\n",
    "        processed_batch = self.process_batch(batch_df)\n",
    "\n",
    "        # Force garbage collection periodically\n",
    "        if self.batch_count % 50 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "        return processed_batch\n",
    "\n",
    "    def _read_parquet_in_batches(self):\n",
    "        \"\"\"Generator to read parquet file in batches\"\"\"\n",
    "        try:\n",
    "            # Use pyarrow \n",
    "            parquet_file = pq.ParquetFile(self.input_file)\n",
    "\n",
    "            # Read in batches using row groups\n",
    "            for batch in parquet_file.iter_batches(batch_size=self.batch_size):\n",
    "                # Convert to pandas DataFrame\n",
    "                df = batch.to_pandas()\n",
    "                yield df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading parquet in batches: {e}\")\n",
    "            # Fallback to pandas chunking\n",
    "            logger.info(\"Falling back to pandas chunking...\")\n",
    "            try:\n",
    "                # Read the entire file and chunk it manually per pandas doesn't support chunksize for parquet\n",
    "                df = pd.read_parquet(self.input_file)\n",
    "                for i in range(0, len(df), self.batch_size):\n",
    "                    yield df.iloc[i:i+self.batch_size]\n",
    "                del df  # Free memory\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Fallback also failed: {e2}\")\n",
    "                raise\n",
    "\n",
    "    def _write_parquet_batch(self, df: pd.DataFrame, is_first_batch: bool = False):\n",
    "        \"\"\"Write batch to parquet file efficiently\"\"\"\n",
    "        if df.empty:\n",
    "            return\n",
    "\n",
    "        # Convert to Arrow Table for efficient writing\n",
    "        table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "\n",
    "        if is_first_batch:\n",
    "            # Create new file\n",
    "            writer = pq.ParquetWriter(\n",
    "                self.output_file,\n",
    "                table.schema,\n",
    "                compression=self.use_compression,\n",
    "                use_dictionary=True  # Enable dictionary encoding\n",
    "            )\n",
    "            self._parquet_writer = writer\n",
    "\n",
    "        # Write the batch\n",
    "        self._parquet_writer.write_table(table)\n",
    "\n",
    "    def run_pipeline(self) -> Dict[str, Any]:\n",
    "        \"\"\"Runs the complete pipeline to add I-95 distance column\"\"\"\n",
    "        logger.info(f\"Starting I-95 distance calculation pipeline: {self.input_file}\")\n",
    "        logger.info(f\"Batch size: {self.batch_size:,}\")\n",
    "        logger.info(f\"Expected batches: {(self.total_rows // self.batch_size) + 1}\")\n",
    "\n",
    "        start_time = pd.Timestamp.now()\n",
    "        first_batch = True\n",
    "        total_rows_saved = 0\n",
    "\n",
    "        try:\n",
    "            # Process file in batches\n",
    "            for batch_df in self._read_parquet_in_batches():\n",
    "                processed_batch = self._process_one_batch(batch_df)\n",
    "                total_rows_saved += len(processed_batch)\n",
    "\n",
    "                self._write_parquet_batch(processed_batch, is_first_batch=first_batch)\n",
    "                first_batch = False\n",
    "\n",
    "            # Close the parquet writer\n",
    "            if hasattr(self, '_parquet_writer'):\n",
    "                self._parquet_writer.close()\n",
    "                logger.info(f\"Saved {total_rows_saved:,} rows to {self.output_file}\")\n",
    "            else:\n",
    "                logger.warning(\"No data to save\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline error: {str(e)}\")\n",
    "            # Clean up partial file\n",
    "            if self.output_file.exists():\n",
    "                self.output_file.unlink()\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if hasattr(self, '_parquet_writer'):\n",
    "                try:\n",
    "                    self._parquet_writer.close()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        end_time = pd.Timestamp.now()\n",
    "        processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'total_rows_processed': self.total_rows_processed,\n",
    "            'total_rows_saved': total_rows_saved,\n",
    "            'total_batches': self.batch_count,\n",
    "            'input_file': str(self.input_file),\n",
    "            'output_file': str(self.output_file),\n",
    "            'input_file_size_gb': self.total_file_size,\n",
    "            'output_file_size_gb': self.output_file.stat().st_size / (1024**3) if self.output_file.exists() else 0,\n",
    "            'processing_time_seconds': processing_time,\n",
    "            'processing_time_formatted': str(pd.Timedelta(seconds=processing_time)),\n",
    "            'rows_per_second': self.total_rows_processed / processing_time if processing_time > 0 else 0,\n",
    "            'compression_ratio': (self.total_file_size / (self.output_file.stat().st_size / (1024**3)))\n",
    "                               if self.output_file.exists() and self.output_file.stat().st_size > 0 else 0\n",
    "        }\n",
    "\n",
    "        logger.info(\"I-95 distance calculation pipeline completed!\")\n",
    "        logger.info(f\"Processing time: {stats['processing_time_formatted']}\")\n",
    "        logger.info(f\"Rows per second: {stats['rows_per_second']:,.0f}\")\n",
    "        logger.info(f\"Compression ratio: {stats['compression_ratio']:.2f}x\")\n",
    "\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea38939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 13:44:57,959 - INFO - Parquet file info:\n",
      "2025-07-18 13:44:57,959 - INFO -   Total rows: 952,074\n",
      "2025-07-18 13:44:57,960 - INFO -   Number of row groups: 1\n",
      "2025-07-18 13:44:57,960 - INFO -   Columns: 52\n",
      "2025-07-18 13:44:57,960 - INFO -   File size: 0.07 GB\n",
      "2025-07-18 13:44:57,960 - INFO - Setting up highway geometry with spatial indexing...\n",
      "2025-07-18 13:44:58,032 - INFO - Highway geometry setup complete\n",
      "2025-07-18 13:44:58,032 - INFO - Starting I-95 distance calculation pipeline: /Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/stations.parquet\n",
      "2025-07-18 13:44:58,032 - INFO - Batch size: 50,000\n",
      "2025-07-18 13:44:58,032 - INFO - Expected batches: 20\n",
      "2025-07-18 13:46:11,341 - INFO - Processing batch 10 (52.5% complete) with 50000 rows\n",
      "2025-07-18 13:47:32,247 - INFO - Processing batch 20 (100.0% complete) with 2074 rows\n",
      "2025-07-18 13:47:32,654 - INFO - Saved 952,074 rows to /Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/birds_with_distances.parquet\n",
      "2025-07-18 13:47:32,658 - INFO - I-95 distance calculation pipeline completed!\n",
      "2025-07-18 13:47:32,658 - INFO - Processing time: 0 days 00:02:34.621570\n",
      "2025-07-18 13:47:32,658 - INFO - Rows per second: 6,157\n",
      "2025-07-18 13:47:32,659 - INFO - Compression ratio: 1.12x\n"
     ]
    }
   ],
   "source": [
    "# Initialize calculator\n",
    "calculator = I95DistanceCalculator(\n",
    "    input_file=\"/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/stations.parquet\",\n",
    "    output_file=\"/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/birds_with_distances.parquet\",\n",
    "    i95_coords=i95_coords,\n",
    "    batch_size=50000,\n",
    "    use_compression='snappy'\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "stats = calculator.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7e2a552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((952074, 16),\n",
       " Index(['TAXON CONCEPT ID', 'COMMON NAME', 'SCIENTIFIC NAME',\n",
       "        'OBSERVATION COUNT', 'STATE CODE', 'COUNTY', 'LOCALITY',\n",
       "        'LOCALITY TYPE', 'LATITUDE', 'LONGITUDE', 'OBSERVATION DATE',\n",
       "        'TIME OBSERVATIONS STARTED', 'DURATION MINUTES', 'station_distances',\n",
       "        'assigned_station', 'i95_distance'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birds = pd.read_parquet(\"/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/birds_with_distances.parquet\")\n",
    "birds.shape, birds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72f452b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NA values:\n",
      "TAXON CONCEPT ID                 0\n",
      "COMMON NAME                      0\n",
      "SCIENTIFIC NAME                  0\n",
      "OBSERVATION COUNT                0\n",
      "STATE CODE                       0\n",
      "COUNTY                           0\n",
      "LOCALITY                         0\n",
      "LOCALITY TYPE                    0\n",
      "LATITUDE                         0\n",
      "LONGITUDE                        0\n",
      "OBSERVATION DATE                 0\n",
      "TIME OBSERVATIONS STARTED     3321\n",
      "DURATION MINUTES             62586\n",
      "station_distances                0\n",
      "assigned_station                 0\n",
      "i95_distance                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "na_count = birds.isna().sum()\n",
    "print(f\"Number of rows with NA values:\\n{na_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe663e",
   "metadata": {},
   "source": [
    "### Add temporal details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "796ef846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'OBSERVATION DATE' to datetime\n",
    "birds['OBSERVATION DATE'] = pd.to_datetime(birds['OBSERVATION DATE'])\n",
    "birds['TIME OBSERVATIONS STARTED'] = pd.to_datetime(birds['TIME OBSERVATIONS STARTED'], format='%H:%M:%S', errors='coerce') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80bdf13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract/Add date components\n",
    "birds[\"year_record\"] = birds[\"OBSERVATION DATE\"].dt.year\n",
    "birds[\"month_record\"] = birds[\"OBSERVATION DATE\"].dt.month\n",
    "birds[\"day_record\"] = birds[\"OBSERVATION DATE\"].dt.day\n",
    "birds[\"day_of_week\"] = birds[\"OBSERVATION DATE\"].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "birds[\"is_weekend\"] = (birds[\"OBSERVATION DATE\"].dt.dayofweek >= 4).astype(int)  # 1 for Friday/Saturday/Sunday, 0 for weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d95d7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation Hour\n",
    "birds['hour_started'] = birds['TIME OBSERVATIONS STARTED'].apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4e5aecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(952074, 23)\n"
     ]
    }
   ],
   "source": [
    "# Migration flag\n",
    "migration_months = [5, 6, 9, 10] \n",
    "birds[\"is_migration\"] = birds[\"month_record\"].isin(migration_months).astype(int)\n",
    "print(birds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4ce9933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((952074, 23),\n",
       " Index(['TAXON CONCEPT ID', 'COMMON NAME', 'SCIENTIFIC NAME',\n",
       "        'OBSERVATION COUNT', 'STATE CODE', 'COUNTY', 'LOCALITY',\n",
       "        'LOCALITY TYPE', 'LATITUDE', 'LONGITUDE', 'OBSERVATION DATE',\n",
       "        'TIME OBSERVATIONS STARTED', 'DURATION MINUTES', 'station_distances',\n",
       "        'assigned_station', 'i95_distance', 'year_record', 'month_record',\n",
       "        'day_record', 'day_of_week', 'is_weekend', 'hour_started',\n",
       "        'is_migration'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birds.shape, birds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50ca3bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "birds.to_parquet('/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/birds_with_flags.parquet', compression='snappy')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72da12d",
   "metadata": {},
   "source": [
    "### Add taxonomy info\n",
    "- \"NorthAmericanBirds.csv\": A file listing North American Bird species - common name, scientific name,  Family and Order - created from wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "783d201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1283, 6)\n",
      "Index(['Text', 'Common_Name', 'Scientific_Name', 'Order', 'Family', 'Class'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "taxo = pd.read_csv(\"/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/NorthAmericanBirds.csv\")\n",
    "print(taxo.shape)\n",
    "print(taxo.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0bbe15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxo = taxo[['Scientific_Name', 'Order']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52627df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(952074, 25)\n",
      "Index(['TAXON CONCEPT ID', 'COMMON NAME', 'SCIENTIFIC NAME',\n",
      "       'OBSERVATION COUNT', 'STATE CODE', 'COUNTY', 'LOCALITY',\n",
      "       'LOCALITY TYPE', 'LATITUDE', 'LONGITUDE', 'OBSERVATION DATE',\n",
      "       'TIME OBSERVATIONS STARTED', 'DURATION MINUTES', 'station_distances',\n",
      "       'assigned_station', 'i95_distance', 'year_record', 'month_record',\n",
      "       'day_record', 'day_of_week', 'is_weekend', 'hour_started',\n",
      "       'is_migration', 'Scientific_Name', 'Order'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "taxo_merged = pd.merge(\n",
    "    birds,\n",
    "    taxo,\n",
    "    left_on=\"SCIENTIFIC NAME\",\n",
    "    right_on=\"Scientific_Name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(taxo_merged.shape)\n",
    "print(taxo_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "732b4f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NA values:\n",
      "TAXON CONCEPT ID                 0\n",
      "COMMON NAME                      0\n",
      "SCIENTIFIC NAME                  0\n",
      "OBSERVATION COUNT                0\n",
      "STATE CODE                       0\n",
      "COUNTY                           0\n",
      "LOCALITY                         0\n",
      "LOCALITY TYPE                    0\n",
      "LATITUDE                         0\n",
      "LONGITUDE                        0\n",
      "OBSERVATION DATE                 0\n",
      "TIME OBSERVATIONS STARTED     3321\n",
      "DURATION MINUTES             62586\n",
      "station_distances                0\n",
      "assigned_station                 0\n",
      "i95_distance                     0\n",
      "year_record                      0\n",
      "month_record                     0\n",
      "day_record                       0\n",
      "day_of_week                      0\n",
      "is_weekend                       0\n",
      "hour_started                  3321\n",
      "is_migration                     0\n",
      "Scientific_Name              53799\n",
      "Order                        53799\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "na_count = taxo_merged.isna().sum()\n",
    "print(f\"Number of rows with NA values:\\n{na_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826cb1c",
   "metadata": {},
   "source": [
    "### Add weather flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "484d9b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US-VA', 'US-ME'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo_merged[\"STATE CODE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a878ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-defined event windows for each state\n",
    "EVENT_WINDOWS = {\n",
    "    'US-ME': [\n",
    "        ('2020-01-11', '2020-01-13'),\n",
    "        ('2020-12-05', '2020-12-06'),\n",
    "        ('2020-12-15', '2020-12-17'),\n",
    "        ('2021-06-07', '2021-06-07'),\n",
    "        ('2023-05-30', '2023-05-30'),\n",
    "        ('2023-06-25', '2023-06-26'),\n",
    "        ('2023-07-18', '2023-07-18'),\n",
    "    ],\n",
    "    'US-VA': [\n",
    "        ('2020-05-28', '2020-05-28'),\n",
    "        ('2020-08-04', '2020-08-04'),\n",
    "        ('2020-12-14', '2020-12-17'),\n",
    "        ('2021-07-09', '2021-07-09'),\n",
    "        ('2021-08-18', '2021-08-18'),\n",
    "        ('2021-09-21', '2021-09-21'),\n",
    "        ('2021-07-21', '2021-07-21'),\n",
    "        ('2022-01-27', '2022-01-29'),\n",
    "        ('2023-03-02', '2023-03-03'),\n",
    "        ('2023-03-31', '2023-04-01'),\n",
    "        ('2023-06-02', '2023-06-07'),\n",
    "        ('2023-09-23', '2023-09-24'),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41325374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_extreme_event(state: str, date_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Check if the given date falls within an extreme weather or poor air quality event for the specified state.\n",
    "\n",
    "    Parameters:\n",
    "    - state: \"Maine\" or \"Virginia\"\n",
    "    - date_str: string in 'YYYY-MM-DD' format\n",
    "\n",
    "    Returns:\n",
    "    - 1 if date is within an event timeframe\n",
    "    - 0 otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "        if state not in EVENT_WINDOWS:\n",
    "            return 0\n",
    "        for start_str, end_str in EVENT_WINDOWS[state]:\n",
    "            start = datetime.strptime(start_str, \"%Y-%m-%d\").date()\n",
    "            end = datetime.strptime(end_str, \"%Y-%m-%d\").date()\n",
    "            if start <= date <= end:\n",
    "                return 1\n",
    "        return 0\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Invalid date format. Use YYYY-MM-DD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03f1d47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TAXON CONCEPT ID', 'COMMON NAME', 'SCIENTIFIC NAME',\n",
      "       'OBSERVATION COUNT', 'STATE CODE', 'COUNTY', 'LOCALITY',\n",
      "       'LOCALITY TYPE', 'LATITUDE', 'LONGITUDE', 'OBSERVATION DATE',\n",
      "       'TIME OBSERVATIONS STARTED', 'DURATION MINUTES', 'station_distances',\n",
      "       'assigned_station', 'i95_distance', 'year_record', 'month_record',\n",
      "       'day_record', 'day_of_week', 'is_weekend', 'hour_started',\n",
      "       'is_migration', 'Scientific_Name', 'Order', 'extreme_weather'],\n",
      "      dtype='object')\n",
      "extreme_weather\n",
      "0    945038\n",
      "1      7036\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# add weather flag and verify the function works\n",
    "taxo_merged['extreme_weather'] = taxo_merged.apply(lambda row: check_extreme_event(row['STATE CODE'], row['OBSERVATION DATE'].strftime('%Y-%m-%d')), axis=1)\n",
    "print(taxo_merged.columns)\n",
    "print(taxo_merged['extreme_weather'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d654c3d",
   "metadata": {},
   "source": [
    "### Key feature clean up - NA rows\n",
    "Previouse EDA showed 41K non-numeric target rows present.<br>\n",
    "TIME OBSERVATION STARTED is a key feature.  Will drop rows if NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7805523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: (952074, 26)\n",
      "After cleaning: (928224, 26)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before cleaning:\", taxo_merged.shape)\n",
    "# Cleaning up 41K nonnumeric data (\"X\") from our target\n",
    "taxo_merged['OBSERVATION COUNT'] = pd.to_numeric(taxo_merged['OBSERVATION COUNT'], errors='coerce')\n",
    "# Dropping non-numeric 'OBSERVATION COUNT' rows\n",
    "taxo_merged = taxo_merged.dropna(subset=['OBSERVATION COUNT', 'TIME OBSERVATIONS STARTED'])\n",
    "print(\"After cleaning:\", taxo_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a4963b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NA values:\n",
      "TAXON CONCEPT ID                 0\n",
      "COMMON NAME                      0\n",
      "SCIENTIFIC NAME                  0\n",
      "OBSERVATION COUNT                0\n",
      "STATE CODE                       0\n",
      "COUNTY                           0\n",
      "LOCALITY                         0\n",
      "LOCALITY TYPE                    0\n",
      "LATITUDE                         0\n",
      "LONGITUDE                        0\n",
      "OBSERVATION DATE                 0\n",
      "TIME OBSERVATIONS STARTED        0\n",
      "DURATION MINUTES             58124\n",
      "station_distances                0\n",
      "assigned_station                 0\n",
      "i95_distance                     0\n",
      "year_record                      0\n",
      "month_record                     0\n",
      "day_record                       0\n",
      "day_of_week                      0\n",
      "is_weekend                       0\n",
      "hour_started                     0\n",
      "is_migration                     0\n",
      "Scientific_Name              52869\n",
      "Order                        52869\n",
      "extreme_weather                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "na_count = taxo_merged.isna().sum()\n",
    "print(f\"Number of rows with NA values:\\n{na_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804d6ba",
   "metadata": {},
   "source": [
    "### Add traffic and noise\n",
    "Requires \"i95_traffic_va_me_small_8am.csv\": A file listing each noise observation stations with the detected daily noise at 8 am in years 2020 through 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ecbd4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387000, 45)\n",
      "Index(['record_type', 'state_code', 'f_system', 'station_id', 'travel_dir',\n",
      "       'travel_lane', 'year_record', 'month_record', 'day_record',\n",
      "       'day_of_week', 'hour_00', 'hour_01', 'hour_02', 'hour_03', 'hour_04',\n",
      "       'hour_05', 'hour_06', 'hour_07', 'hour_08', 'hour_09', 'hour_10',\n",
      "       'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16',\n",
      "       'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22',\n",
      "       'hour_23', 'restrictions', 'latitude', 'longitude', 'station_location',\n",
      "       'state', 'daily_avg_noise', 'peak_hour_noise', 'overnight_noise',\n",
      "       '8am_noise', 'rush_hour_noise', 'total_daily_volume'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "traffic = pd.read_csv('/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/i95_traffic_va_me_small_8am.csv', \n",
    "                      dtype={'station_id': str})\n",
    "print(traffic.shape)\n",
    "print(traffic.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bf56196",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = traffic[['state_code', 'station_id', 'year_record', 'month_record', 'day_record',\n",
    "       'daily_avg_noise', 'peak_hour_noise', 'overnight_noise', \n",
    "       'rush_hour_noise', 'total_daily_volume', '8am_noise']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "922ea729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year_record\n",
       "2022    98813\n",
       "2023    97496\n",
       "21      96216\n",
       "2020    94475\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic.year_record.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7b8fdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387000, 11)\n",
      "\n",
      "DataFrame after replacement (Method 1 - Boolean Indexing):\n",
      "(387000, 11)\n"
     ]
    }
   ],
   "source": [
    "print(traffic.shape)\n",
    "# Find year_record = 21\n",
    "condition = traffic['year_record'] == 21\n",
    "\n",
    "# Select the rows and the specific column, then assign the new value\n",
    "traffic.loc[condition, 'year_record'] = 2021\n",
    "\n",
    "print(\"\\nDataFrame after replacement (Method 1 - Boolean Indexing):\")\n",
    "print(traffic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87e1248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year_record\n",
       "2022    98813\n",
       "2023    97496\n",
       "2021    96216\n",
       "2020    94475\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic.year_record.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "439abd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NA values:\n",
      "state_code               0\n",
      "station_id               0\n",
      "year_record              0\n",
      "month_record             0\n",
      "day_record               0\n",
      "daily_avg_noise        141\n",
      "peak_hour_noise        141\n",
      "overnight_noise        295\n",
      "rush_hour_noise        168\n",
      "total_daily_volume       0\n",
      "8am_noise             1593\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "na_count = traffic.isna().sum()\n",
    "print(f\"Number of rows with NA values:\\n{na_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "163422dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((928224, 26),\n",
       " Index(['TAXON CONCEPT ID', 'COMMON NAME', 'SCIENTIFIC NAME',\n",
       "        'OBSERVATION COUNT', 'STATE CODE', 'COUNTY', 'LOCALITY',\n",
       "        'LOCALITY TYPE', 'LATITUDE', 'LONGITUDE', 'OBSERVATION DATE',\n",
       "        'TIME OBSERVATIONS STARTED', 'DURATION MINUTES', 'station_distances',\n",
       "        'assigned_station', 'i95_distance', 'year_record', 'month_record',\n",
       "        'day_record', 'day_of_week', 'is_weekend', 'hour_started',\n",
       "        'is_migration', 'Scientific_Name', 'Order', 'extreme_weather'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo_merged.shape, taxo_merged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c6ede",
   "metadata": {},
   "source": [
    "Resolving traffic['station'] and taxo_merged['assigned_station'] values mismatch by adding leading 0s to traffic['station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9f4ff61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station\n",
       "030209    4334\n",
       "030210    4335\n",
       "040007    1431\n",
       "040046    1459\n",
       "040052    1392\n",
       "          ... \n",
       "840062    1437\n",
       "840066    1429\n",
       "940050    1438\n",
       "940447    1439\n",
       "940762    1427\n",
       "Name: count, Length: 254, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic['station'] = traffic[['station_id']].astype(str)\n",
    "traffic['station'] = traffic['station'].str.zfill(6)\n",
    "traffic['station'].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb82cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxo_merged[['assigned_station']]=taxo_merged[['assigned_station']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6798baee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common stations: 33\n"
     ]
    }
   ],
   "source": [
    "common = set(traffic['station']).intersection(taxo_merged['assigned_station'])\n",
    "print(f\"Common stations: {len(common)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3a91bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic duplicates: station  year_record  month_record  day_record\n",
      "030209   2020         1             1             3\n",
      "                                    2             3\n",
      "                                    3             3\n",
      "                                    4             3\n",
      "                                    5             3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check duplicates in traffic\n",
    "traffic_dupes = traffic.groupby(['station', 'year_record', 'month_record', 'day_record']).size()\n",
    "print(\"Traffic duplicates:\", traffic_dupes[traffic_dupes > 1].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b234e4",
   "metadata": {},
   "source": [
    "Traffic data hs duplicate rows.  Removing duplicates before merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c335de49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(335335, 12)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_deduped = traffic.drop_duplicates(\n",
    "    subset=['station', 'year_record', 'month_record', 'day_record'])\n",
    "traffic_deduped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2199d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before merge: (928224, 26)\n",
      "after merge: (928224, 35)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['TAXON CONCEPT ID', 'COMMON NAME', 'SCIENTIFIC NAME',\n",
       "       'OBSERVATION COUNT', 'STATE CODE', 'COUNTY', 'LOCALITY',\n",
       "       'LOCALITY TYPE', 'LATITUDE', 'LONGITUDE', 'OBSERVATION DATE',\n",
       "       'TIME OBSERVATIONS STARTED', 'DURATION MINUTES', 'station_distances',\n",
       "       'assigned_station', 'i95_distance', 'year_record', 'month_record',\n",
       "       'day_record', 'day_of_week', 'is_weekend', 'hour_started',\n",
       "       'is_migration', 'Scientific_Name', 'Order', 'extreme_weather',\n",
       "       'state_code', 'station_id', 'daily_avg_noise', 'peak_hour_noise',\n",
       "       'overnight_noise', 'rush_hour_noise', 'total_daily_volume', '8am_noise',\n",
       "       'station'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"before merge:\", taxo_merged.shape)\n",
    "merged_df = pd.merge(\n",
    "    taxo_merged,\n",
    "    traffic_deduped,\n",
    "    left_on=['assigned_station', 'year_record', 'month_record', 'day_record'],\n",
    "    right_on=['station', 'year_record', 'month_record', 'day_record'],\n",
    "    how=\"left\"\n",
    ")\n",
    "print(\"after merge:\", merged_df.shape)\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf6816",
   "metadata": {},
   "source": [
    "Clean up duplicative colums created during the merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bad8599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(928224, 28)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df[['OBSERVATION COUNT','COMMON NAME', 'SCIENTIFIC NAME', 'Order',\n",
    "        'STATE CODE', 'COUNTY', 'LOCALITY TYPE', 'LATITUDE', 'LONGITUDE', \n",
    "        'TIME OBSERVATIONS STARTED', 'DURATION MINUTES', 'i95_distance', \n",
    "        'station_distances', 'assigned_station', 'year_record', 'month_record', \n",
    "        'day_record', 'hour_started', 'day_of_week', 'is_weekend', 'is_migration', \n",
    "        'extreme_weather', 'daily_avg_noise', 'peak_hour_noise', 'overnight_noise', \n",
    "        'rush_hour_noise', 'total_daily_volume', '8am_noise',\n",
    "       ]]\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eab930d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NA values:\n",
      "OBSERVATION COUNT                 0\n",
      "COMMON NAME                       0\n",
      "SCIENTIFIC NAME                   0\n",
      "Order                         52869\n",
      "STATE CODE                        0\n",
      "COUNTY                            0\n",
      "LOCALITY TYPE                     0\n",
      "LATITUDE                          0\n",
      "LONGITUDE                         0\n",
      "TIME OBSERVATIONS STARTED         0\n",
      "DURATION MINUTES              58124\n",
      "i95_distance                      0\n",
      "station_distances                 0\n",
      "assigned_station                  0\n",
      "year_record                       0\n",
      "month_record                      0\n",
      "day_record                        0\n",
      "hour_started                      0\n",
      "day_of_week                       0\n",
      "is_weekend                        0\n",
      "is_migration                      0\n",
      "extreme_weather                   0\n",
      "daily_avg_noise              212923\n",
      "peak_hour_noise              212923\n",
      "overnight_noise              212959\n",
      "rush_hour_noise              212942\n",
      "total_daily_volume           212923\n",
      "8am_noise                    213044\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "na_count = merged_df.isna().sum()\n",
    "print(f\"Number of rows with NA values:\\n{na_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ecc711",
   "metadata": {},
   "source": [
    "8am_noise is a key feature.  Dropping rows if 8am_noise is NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6fe0092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DF Shape: (715180, 28)\n",
      "Number of rows with NA values:\n",
      "OBSERVATION COUNT                0\n",
      "COMMON NAME                      0\n",
      "SCIENTIFIC NAME                  0\n",
      "Order                        40480\n",
      "STATE CODE                       0\n",
      "COUNTY                           0\n",
      "LOCALITY TYPE                    0\n",
      "LATITUDE                         0\n",
      "LONGITUDE                        0\n",
      "TIME OBSERVATIONS STARTED        0\n",
      "DURATION MINUTES             46723\n",
      "i95_distance                     0\n",
      "station_distances                0\n",
      "assigned_station                 0\n",
      "year_record                      0\n",
      "month_record                     0\n",
      "day_record                       0\n",
      "hour_started                     0\n",
      "day_of_week                      0\n",
      "is_weekend                       0\n",
      "is_migration                     0\n",
      "extreme_weather                  0\n",
      "daily_avg_noise                  0\n",
      "peak_hour_noise                  0\n",
      "overnight_noise                 28\n",
      "rush_hour_noise                  0\n",
      "total_daily_volume               0\n",
      "8am_noise                        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = merged_df.dropna(subset=['8am_noise'])\n",
    "print('Final DF Shape:', cleaned_df.shape)\n",
    "na_count = cleaned_df.isna().sum()\n",
    "print(f\"Number of rows with NA values:\\n{na_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7321090a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(715180, 28)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3dbe93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_parquet('/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/birds_test_ready_no_filter.parquet', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc1068f",
   "metadata": {},
   "source": [
    "### Creating test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34afca8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame shape (8 AM observations): (100387, 28)\n",
      "Filtered DataFrame shape (8 AM weekend observations): (52106, 28)\n",
      "Filtered DataFrame shape (weekend observations): (349112, 28)\n"
     ]
    }
   ],
   "source": [
    "filtered_8am = cleaned_df[cleaned_df['hour_started'] == 8]\n",
    "print(\"Filtered DataFrame shape (8 AM observations):\", filtered_8am.shape)\n",
    "filtered_8am.to_parquet('/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/birds_8am.parquet', compression='snappy')\n",
    "\n",
    "filtered_8am_weekend = filtered_8am[filtered_8am['is_weekend'] == 1]\n",
    "print(\"Filtered DataFrame shape (8 AM weekend observations):\", filtered_8am_weekend.shape)  \n",
    "filtered_8am_weekend.to_parquet('/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/birds_8am_weekend.parquet', compression='snappy')\n",
    "\n",
    "filtered_weekend = cleaned_df[cleaned_df['is_weekend'] == 1]\n",
    "print(\"Filtered DataFrame shape (weekend observations):\", filtered_weekend.shape)\n",
    "filtered_weekend.to_parquet('/Users/sooneui/Documents/UCB_MIDS/Data/Capstone_Data/ETL/birds_weekend.parquet', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33082c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OBSERVATION COUNT', 'COMMON NAME', 'SCIENTIFIC NAME', 'Order',\n",
       "       'STATE CODE', 'COUNTY', 'LOCALITY TYPE', 'LATITUDE', 'LONGITUDE',\n",
       "       'TIME OBSERVATIONS STARTED', 'DURATION MINUTES', 'i95_distance',\n",
       "       'station_distances', 'assigned_station', 'year_record', 'month_record',\n",
       "       'day_record', 'hour_started', 'day_of_week', 'is_weekend',\n",
       "       'is_migration', 'extreme_weather', 'daily_avg_noise', 'peak_hour_noise',\n",
       "       'overnight_noise', 'rush_hour_noise', 'total_daily_volume',\n",
       "       '8am_noise'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83cb7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df = cleaned_df.groupby(['assigned_station', 'year_record', 'month_record', 'day_record'])['OBSERVATION COUNT'].sum().reset_index()\n",
    "aggregated_df_weekend = filtered_weekend.groupby(['assigned_station', 'year_record', 'month_record', 'day_record'])['OBSERVATION COUNT'].sum().reset_index()\n",
    "aggregated_df_8am = filtered_8am.groupby(['assigned_station', 'year_record', 'month_record', 'day_record'])['OBSERVATION COUNT'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff5d2c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregated_df (unfiltered): (22735, 5)\n",
      "aggregated_df_weekend: (10251, 5)\n",
      "aggregated_df_8am: (8958, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"aggregated_df (unfiltered):\", aggregated_df.shape)\n",
    "print(\"aggregated_df_weekend:\", aggregated_df_weekend.shape)\n",
    "print(\"aggregated_df_8am:\", aggregated_df_8am.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14436d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2020, 2021, 2022, 2023], dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.year_record.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
